# TinyAI ä¸–ç•Œæ¨¡å‹å…·èº«æ™ºèƒ½æ¨¡å—ï¼ˆWorld Modelï¼‰

## ğŸ“– æ¨¡å—ç®€ä»‹

`tinyai-agent-embodied-wm` æ˜¯ TinyAI æ™ºèƒ½ä½“ç³»ç»Ÿå±‚çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¸“æ³¨äº**åŸºäºä¸–ç•Œæ¨¡å‹çš„å…·èº«æ™ºèƒ½**ï¼ˆWorld Model-based Embodied Intelligenceï¼‰æŠ€æœ¯å®ç°ã€‚æœ¬æ¨¡å—å®ç°äº†å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹æ¶æ„ï¼Œå±•ç¤ºäº†æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡å­¦ä¹ ç¯å¢ƒçš„å†…éƒ¨è¡¨ç¤ºæ¥è¿›è¡Œé«˜æ•ˆçš„ç«¯åˆ°ç«¯å­¦ä¹ å’Œè§„åˆ’ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ§  **å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹æ¶æ„**ï¼šVAEç¼–ç å™¨ + MDN-RNNè®°å¿† + æ§åˆ¶å™¨
- ğŸ¯ **ç«¯åˆ°ç«¯å­¦ä¹ èƒ½åŠ›**ï¼šä»åŸå§‹è§‚å¯Ÿç›´æ¥åˆ°åŠ¨ä½œå†³ç­–
- ğŸ’­ **æƒ³è±¡è®­ç»ƒ**ï¼šåœ¨å†…éƒ¨æ¨¡å‹ä¸­è¿›è¡Œè§„åˆ’ï¼Œæ— éœ€çœŸå®ç¯å¢ƒäº¤äº’
- ğŸ”„ **é«˜æ•ˆæ ·æœ¬åˆ©ç”¨**ï¼šé€šè¿‡æƒ³è±¡rolloutæé«˜æ•°æ®æ•ˆç‡
- ğŸ¨ **å¯æ‰©å±•è®¾è®¡**ï¼šæ¨¡å—åŒ–æ¶æ„ï¼Œä¾¿äºå®šåˆ¶å’Œæ‰©å±•

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### ä¸–ç•Œæ¨¡å‹ä¸‰å¤§æ ¸å¿ƒç»„ä»¶

```
ä¸–ç•Œæ¨¡å‹æ¶æ„
â”œâ”€â”€ VAEç¼–ç å™¨ï¼ˆVision Componentï¼‰
â”‚   â”œâ”€â”€ ç¼–ç å™¨ï¼šObservation -> (Î¼, ÏƒÂ²)
â”‚   â”œâ”€â”€ é‡å‚æ•°åŒ–ï¼šz = Î¼ + ÏƒÂ·Îµ
â”‚   â””â”€â”€ è§£ç å™¨ï¼šz -> Reconstructed Observation
â”‚
â”œâ”€â”€ MDN-RNNï¼ˆMemory Componentï¼‰
â”‚   â”œâ”€â”€ GRUå•å…ƒï¼šç»´æŠ¤æ—¶åºè®°å¿†
â”‚   â”œâ”€â”€ æ··åˆå¯†åº¦ç½‘ç»œï¼šé¢„æµ‹ä¸‹ä¸€çŠ¶æ€åˆ†å¸ƒ
â”‚   â””â”€â”€ é‡‡æ ·å™¨ï¼šä»åˆ†å¸ƒä¸­é‡‡æ ·ä¸‹ä¸€çŠ¶æ€
â”‚
â””â”€â”€ æ§åˆ¶å™¨ï¼ˆControllerï¼‰
    â”œâ”€â”€ ç­–ç•¥ç½‘ç»œï¼š(z, h) -> action
    â”œâ”€â”€ æ¢ç´¢ç­–ç•¥ï¼šæ·»åŠ å™ªå£°è¿›è¡Œæ¢ç´¢
    â””â”€â”€ ä¼˜åŒ–å™¨ï¼šCMA-ESç­‰è¿›åŒ–ç®—æ³•
```

### å®Œæ•´å·¥ä½œæµç¨‹

```
1. æ„ŸçŸ¥é˜¶æ®µ
   è§‚å¯Ÿ -> VAEç¼–ç å™¨ -> æ½œåœ¨çŠ¶æ€z

2. è®°å¿†é˜¶æ®µ
   (z_t, a_t, h_t) -> MDN-RNN -> (z_{t+1}, h_{t+1})

3. å†³ç­–é˜¶æ®µ
   (z, h) -> æ§åˆ¶å™¨ -> åŠ¨ä½œa

4. æƒ³è±¡é˜¶æ®µï¼ˆå¯é€‰ï¼‰
   å†…éƒ¨æ¨¡å‹ -> æƒ³è±¡rollout -> è®­ç»ƒæ•°æ®
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

- JDK 17 æˆ–æ›´é«˜ç‰ˆæœ¬
- Maven 3.6+

### 2. ç¼–è¯‘æ¨¡å—

```bash
cd /path/to/TinyAI
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home
mvn clean compile -pl tinyai-agent-embodied-wm -am
```

### 3. è¿è¡Œæ¼”ç¤ºç¨‹åº

```bash
mvn exec:java -Dexec.mainClass="io.leavesfly.tinyai.wm.WorldModelDemo" \
              -pl tinyai-agent-embodied-wm
```

### 4. åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

#### ç¤ºä¾‹1ï¼šåˆ›å»ºä¸–ç•Œæ¨¡å‹

```java
// 1. åˆ›å»ºä¸–ç•Œæ¨¡å‹é…ç½®
WorldModel.WorldModelConfig config = new WorldModel.WorldModelConfig(
    64,    // è§‚å¯Ÿç©ºé—´ç»´åº¦
    32,    // æ½œåœ¨ç©ºé—´ç»´åº¦
    256,   // éšè—çŠ¶æ€ç»´åº¦
    3,     // åŠ¨ä½œç©ºé—´ç»´åº¦
    128,   // VAEéšè—å±‚ç»´åº¦
    5,     // æ··åˆé«˜æ–¯åˆ†é‡æ•°
    false  // æ˜¯å¦ç¡®å®šæ€§ç­–ç•¥
);

// 2. åˆ›å»ºä¸–ç•Œæ¨¡å‹
WorldModel worldModel = new WorldModel(config);
```

#### ç¤ºä¾‹2ï¼šä½¿ç”¨æ™ºèƒ½ä½“äº¤äº’

```java
// 1. åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
Environment env = new SimpleDrivingEnvironment();
WorldModelAgent agent = new WorldModelAgent(worldModel, env);

// 2. è¿è¡Œæƒ…æ™¯
Episode episode = agent.runEpisode(1000);

System.out.println("æƒ…æ™¯é•¿åº¦: " + episode.getLength());
System.out.println("æ€»å¥–åŠ±: " + episode.getTotalReward());
```

#### ç¤ºä¾‹3ï¼šæƒ³è±¡è®­ç»ƒ

```java
// 1. åœ¨çœŸå®ç¯å¢ƒä¸­æ”¶é›†åˆå§‹ç»éªŒ
agent.reset();
for (int i = 0; i < 100; i++) {
    agent.step();
}

// 2. åœ¨æƒ³è±¡ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒ
Episode dreamEpisode = agent.trainInDream(500);

System.out.println("æƒ³è±¡æƒ…æ™¯å¥–åŠ±: " + dreamEpisode.getTotalReward());
```

## ğŸ“Š æ ¸å¿ƒç»„ä»¶è¯´æ˜

### 1. æ•°æ®æ¨¡å‹

| ç±»å | è¯´æ˜ | ä¸»è¦å­—æ®µ |
|------|------|---------|
| `Observation` | ç¯å¢ƒè§‚å¯Ÿ | visualObservation, stateVector |
| `Action` | æ™ºèƒ½ä½“åŠ¨ä½œ | actionVector, actionType |
| `LatentState` | æ½œåœ¨çŠ¶æ€ | z, mu, logVar |
| `HiddenState` | RNNéšè—çŠ¶æ€ | h, c (LSTM) |
| `WorldModelState` | ä¸–ç•Œæ¨¡å‹çŠ¶æ€ | latentState, hiddenState |
| `Transition` | çŠ¶æ€è½¬æ¢ | observation, action, reward, nextObservation |
| `Episode` | æƒ…æ™¯è®°å½• | transitions, totalReward |

### 2. VAEç¼–ç å™¨

**åŠŸèƒ½**ï¼šå°†é«˜ç»´è§‚å¯Ÿå‹ç¼©ä¸ºä½ç»´æ½œåœ¨è¡¨ç¤º

**ç½‘ç»œç»“æ„**ï¼š
```
ç¼–ç å™¨ï¼š
  Input(observationSize) 
  -> Linear(hiddenSize) + ReLU
  -> Linear(hiddenSize) + ReLU
  -> [Î¼_layer(latentSize), ÏƒÂ²_layer(latentSize)]

é‡å‚æ•°åŒ–ï¼š
  z = Î¼ + ÏƒÂ·Îµ, Îµ ~ N(0,1)

è§£ç å™¨ï¼š
  Input(latentSize)
  -> Linear(hiddenSize) + ReLU
  -> Linear(hiddenSize) + ReLU
  -> Linear(observationSize)
```

**æŸå¤±å‡½æ•°**ï¼š
```
L_VAE = L_recon + L_KL
L_recon = ||x - x_reconstructed||Â²
L_KL = -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
```

### 3. MDN-RNN

**åŠŸèƒ½**ï¼šé¢„æµ‹æ½œåœ¨çŠ¶æ€çš„æ—¶åºæ¼”åŒ–

**ç½‘ç»œç»“æ„**ï¼š
```
è¾“å…¥å¤„ç†ï¼š
  [z_t; a_t] -> Linear(hiddenSize)

GRUå•å…ƒï¼š
  reset_gate = Ïƒ(W_r * [input; h])
  update_gate = Ïƒ(W_z * [input; h])
  candidate = tanh(W * [input; râŠ™h])
  h_new = (1-z)âŠ™h + zâŠ™h_tilde

MDNè¾“å‡ºï¼š
  h -> [weights, Î¼, Ïƒ] (æ··åˆé«˜æ–¯å‚æ•°)
```

**æŸå¤±å‡½æ•°**ï¼š
```
L_MDN = -log(Î£ Ï€_i Â· N(z_{t+1}|Î¼_i, Ïƒ_iÂ²))
```

### 4. æ§åˆ¶å™¨

**åŠŸèƒ½**ï¼šåŸºäºä¸–ç•Œæ¨¡å‹çŠ¶æ€é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ

**ç½‘ç»œç»“æ„**ï¼š
```
[z; h] 
-> Linear(64) + ReLU
-> Linear(32) + ReLU
-> Linear(actionSize) + Tanh
-> action âˆˆ [-1, 1]^actionSize
```

**è®­ç»ƒæ–¹æ³•**ï¼š
- CMA-ESï¼ˆåæ–¹å·®çŸ©é˜µè‡ªé€‚åº”è¿›åŒ–ç­–ç•¥ï¼‰
- åœ¨æƒ³è±¡ç¯å¢ƒä¸­è¯„ä¼°é€‚åº”åº¦
- æ— éœ€æ¢¯åº¦ï¼Œé€‚åˆå°è§„æ¨¡æ§åˆ¶å™¨

## ğŸ¯ æŠ€æœ¯äº®ç‚¹

### 1. åˆ†ç¦»å¼å­¦ä¹ 

ä¸–ç•Œæ¨¡å‹é‡‡ç”¨åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥ï¼š

1. **é˜¶æ®µä¸€ï¼šè®­ç»ƒVAE**
   - æ”¶é›†è§‚å¯Ÿæ•°æ®
   - è®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨
   - å­¦ä¹ å‹ç¼©çš„æ½œåœ¨è¡¨ç¤º

2. **é˜¶æ®µäºŒï¼šè®­ç»ƒMDN-RNN**
   - åœ¨æ½œåœ¨ç©ºé—´ä¸­æ”¶é›†åºåˆ—
   - è®­ç»ƒé¢„æµ‹ä¸‹ä¸€çŠ¶æ€çš„RNN
   - å­¦ä¹ ç¯å¢ƒåŠ¨æ€æ¨¡å‹

3. **é˜¶æ®µä¸‰ï¼šè®­ç»ƒæ§åˆ¶å™¨**
   - åœ¨æƒ³è±¡ç¯å¢ƒä¸­è¿›è¡Œrollout
   - ä½¿ç”¨è¿›åŒ–ç®—æ³•ä¼˜åŒ–ç­–ç•¥
   - æ— éœ€çœŸå®ç¯å¢ƒäº¤äº’

### 2. æƒ³è±¡è®­ç»ƒ

æ™ºèƒ½ä½“å¯ä»¥å®Œå…¨åœ¨å†…éƒ¨æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒï¼š

```java
// æƒ³è±¡rolloutæµç¨‹
for (int t = 0; t < dreamSteps; t++) {
    // 1. æ§åˆ¶å™¨é€‰æ‹©åŠ¨ä½œ
    action = controller.selectAction(state);
    
    // 2. MDN-RNNé¢„æµ‹ä¸‹ä¸€çŠ¶æ€
    nextState = mdnRnn.predict(state, action);
    
    // 3. è®¡ç®—æƒ³è±¡å¥–åŠ±
    reward = calculateImaginedReward(state, action, nextState);
    
    // 4. æ›´æ–°æ§åˆ¶å™¨
    updateController(reward);
}
```

**ä¼˜åŠ¿**ï¼š
- æ ·æœ¬æ•ˆç‡é«˜ï¼šæ— éœ€å¤§é‡çœŸå®ç¯å¢ƒäº¤äº’
- è®­ç»ƒé€Ÿåº¦å¿«ï¼šå†…éƒ¨æ¨¡å‹è¿è¡Œé€Ÿåº¦è¿œè¶…çœŸå®ç¯å¢ƒ
- å®‰å…¨æ€§å¥½ï¼šé¿å…åœ¨çœŸå®ç¯å¢ƒä¸­çš„å±é™©æ¢ç´¢

### 3. é«˜æ–¯æ··åˆå¯†åº¦ç½‘ç»œ

ä½¿ç”¨æ··åˆé«˜æ–¯åˆ†å¸ƒå»ºæ¨¡çŠ¶æ€è½¬æ¢çš„éšæœºæ€§ï¼š

```
p(z_{t+1}|z_t, a_t, h_t) = Î£ Ï€_i(h_t) Â· N(Î¼_i(h_t), Ïƒ_iÂ²(h_t))
```

**ä¼˜åŠ¿**ï¼š
- å¯ä»¥è¡¨ç¤ºå¤šæ¨¡æ€åˆ†å¸ƒ
- æ•è·ç¯å¢ƒçš„éšæœºæ€§
- æ¯”å•ä¸€é«˜æ–¯æ›´çµæ´»

## ğŸ“š ä¾èµ–å…³ç³»

æœ¬æ¨¡å—ä¾èµ–ä»¥ä¸‹TinyAIæ ¸å¿ƒæ¨¡å—ï¼š

```xml
<dependencies>
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-ndarr</artifactId>
    </dependency>
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-func</artifactId>
    </dependency>
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-nnet</artifactId>
    </dependency>
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-ml</artifactId>
    </dependency>
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-rl</artifactId>
    </dependency>
</dependencies>
```

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [**æŠ€æœ¯æ¶æ„æ–‡æ¡£**](doc/æŠ€æœ¯æ¶æ„æ–‡æ¡£.md) - è¯¦ç»†çš„ç³»ç»Ÿè®¾è®¡æ–‡æ¡£
- [**TinyAI ä¸»æ–‡æ¡£**](../README.md) - é¡¹ç›®æ€»ä½“ä»‹ç»
- [**å…·èº«æ™ºèƒ½æ¨¡å—**](../tinyai-agent-embodied/README.md) - ç›¸å…³æ¨¡å—å‚è€ƒ

## ğŸ“ å­¦ä¹ è·¯å¾„

å»ºè®®æŒ‰ç…§ä»¥ä¸‹é¡ºåºå­¦ä¹ æœ¬æ¨¡å—ï¼š

1. **ç†è®ºåŸºç¡€** - äº†è§£ä¸–ç•Œæ¨¡å‹çš„åŸºæœ¬åŸç†å’Œè®ºæ–‡
2. **VAEç¼–ç ** - å­¦ä¹ å˜åˆ†è‡ªç¼–ç å™¨çš„å®ç°
3. **MDN-RNN** - ç†è§£æ··åˆå¯†åº¦ç½‘ç»œå’ŒRNNè®°å¿†
4. **æ§åˆ¶å™¨** - æŒæ¡ç­–ç•¥ç½‘ç»œå’Œè¿›åŒ–ç®—æ³•
5. **æƒ³è±¡è®­ç»ƒ** - å®è·µåœ¨å†…éƒ¨æ¨¡å‹ä¸­è®­ç»ƒ

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ

### ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelï¼‰

ä¸–ç•Œæ¨¡å‹æ˜¯æ™ºèƒ½ä½“å¯¹ç¯å¢ƒçš„å†…éƒ¨è¡¨ç¤ºï¼ŒåŒ…å«ï¼š

1. **è§†è§‰æ¨¡å‹ï¼ˆVï¼‰**ï¼šVAEç¼–ç å™¨
   - å‹ç¼©é«˜ç»´æ„ŸçŸ¥åˆ°ä½ç»´æ½œåœ¨ç©ºé—´
   - å­¦ä¹ ç¯å¢ƒçš„è§†è§‰ç‰¹å¾

2. **è®°å¿†æ¨¡å‹ï¼ˆMï¼‰**ï¼šMDN-RNN
   - é¢„æµ‹ç¯å¢ƒçš„æ—¶åºåŠ¨æ€
   - ç»´æŠ¤å†å²ä¿¡æ¯

3. **æ§åˆ¶å™¨ï¼ˆCï¼‰**ï¼šç­–ç•¥ç½‘ç»œ
   - åŸºäºå‹ç¼©è¡¨ç¤ºåšå†³ç­–
   - å¯åœ¨æƒ³è±¡ä¸­è®­ç»ƒ

### ç«¯åˆ°ç«¯å­¦ä¹ 

ç›´æ¥ä»åŸå§‹è§‚å¯Ÿå­¦ä¹ åˆ°åŠ¨ä½œæ˜ å°„ï¼š

```
Raw Observation -> VAE -> Latent z -> Controller -> Action
```

**ä¼˜åŠ¿**ï¼š
- æ— éœ€æ‰‹å·¥ç‰¹å¾å·¥ç¨‹
- ç«¯åˆ°ç«¯ä¼˜åŒ–æ•´ä¸ªæµç¨‹
- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

## ğŸ”¬ æŠ€æœ¯å‚æ•°

### é»˜è®¤é…ç½®

```java
observationSize = 64     // è§‚å¯Ÿå‘é‡ç»´åº¦
latentSize = 32         // æ½œåœ¨ç©ºé—´ç»´åº¦
hiddenSize = 256        // RNNéšè—çŠ¶æ€ç»´åº¦
actionSize = 3          // åŠ¨ä½œç©ºé—´ç»´åº¦
vaeHiddenSize = 128     // VAEéšè—å±‚ç»´åº¦
numMixtures = 5         // æ··åˆé«˜æ–¯åˆ†é‡æ•°
deterministic = false   // éšæœºç­–ç•¥
```

### è®­ç»ƒè¶…å‚æ•°

```java
// VAEè®­ç»ƒ
vaeLearningRate = 0.001
vaeEpochs = 100
vaeBatchSize = 32

// MDN-RNNè®­ç»ƒ
rnnLearningRate = 0.001
rnnEpochs = 50
rnnSequenceLength = 32

// æ§åˆ¶å™¨è®­ç»ƒï¼ˆCMA-ESï¼‰
populationSize = 16
sigma = 0.1
generations = 100
```

## ğŸ§ª ä»£ç ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```java
// 1. åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
Environment env = new SimpleDrivingEnvironment();
WorldModel worldModel = new WorldModel(WorldModel.WorldModelConfig.createDefault());
WorldModelAgent agent = new WorldModelAgent(worldModel, env);

// 2. æ”¶é›†è®­ç»ƒæ•°æ®ï¼ˆçœŸå®ç¯å¢ƒï¼‰
List<Episode> realEpisodes = new ArrayList<>();
for (int i = 0; i < 100; i++) {
    Episode episode = agent.runEpisode(1000);
    realEpisodes.add(episode);
}

// 3. è®­ç»ƒVAEï¼ˆç¦»çº¿ï¼‰
trainVAE(worldModel.getVaeEncoder(), realEpisodes);

// 4. è®­ç»ƒMDN-RNNï¼ˆç¦»çº¿ï¼‰
trainMDNRNN(worldModel.getMdnRnn(), realEpisodes);

// 5. è®­ç»ƒæ§åˆ¶å™¨ï¼ˆæƒ³è±¡ç¯å¢ƒï¼‰
for (int i = 0; i < 1000; i++) {
    Episode dreamEpisode = agent.trainInDream(100);
    updateController(worldModel.getController(), dreamEpisode);
}

// 6. è¯„ä¼°æ€§èƒ½
double avgReward = agent.evaluate(10);
System.out.println("å¹³å‡å¥–åŠ±: " + avgReward);
```

## ğŸ“Š æ¨¡å—ç»Ÿè®¡

| ç±»åˆ« | æ•°é‡ | è¯´æ˜ |
|-----|------|------|
| Java ç±»æ–‡ä»¶ | 15+ | åŒ…æ‹¬æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ |
| æ•°æ®æ¨¡å‹ | 8ä¸ª | è§‚å¯Ÿã€åŠ¨ä½œã€çŠ¶æ€ç­‰ |
| æ ¸å¿ƒç»„ä»¶ | 3ä¸ª | VAEã€MDN-RNNã€Controller |
| ç¯å¢ƒå®ç° | 1ä¸ª | ç®€å•é©¾é©¶ç¯å¢ƒ |
| æ¼”ç¤ºç¨‹åº | 1ä¸ª | å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ |

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

| é¡¹ç›® | ç‰ˆæœ¬/é…ç½® | è¯´æ˜ |
|-----|----------|------|
| Java | JDK 17+ | æ ¸å¿ƒè¯­è¨€ |
| Maven | 3.6+ | æ„å»ºå·¥å…· |
| TinyAI NdArray | 1.0.0 | å¤šç»´æ•°ç»„åº“ |
| TinyAI AutoGrad | 1.0.0 | è‡ªåŠ¨å¾®åˆ† |
| TinyAI NeuralNet | 1.0.0 | ç¥ç»ç½‘ç»œ |

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸–ç•Œæ¨¡å‹ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: ä¸–ç•Œæ¨¡å‹å­¦ä¹ ç¯å¢ƒçš„å†…éƒ¨è¡¨ç¤ºï¼Œå¯ä»¥åœ¨æƒ³è±¡ä¸­è®­ç»ƒï¼Œå¤§å¤§æé«˜æ ·æœ¬æ•ˆç‡ã€‚ä¼ ç»ŸRLéœ€è¦å¤§é‡çœŸå®ç¯å¢ƒäº¤äº’ã€‚

### Q2: ä¸ºä»€ä¹ˆä½¿ç”¨æ··åˆå¯†åº¦ç½‘ç»œï¼Ÿ

**A**: ç¯å¢ƒè½¬æ¢å¾€å¾€æ˜¯éšæœºçš„å’Œå¤šæ¨¡æ€çš„ï¼Œå•ä¸€é«˜æ–¯æ— æ³•å¾ˆå¥½å»ºæ¨¡ã€‚MDNå¯ä»¥è¡¨ç¤ºå¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒã€‚

### Q3: å¦‚ä½•è°ƒæ•´æ½œåœ¨ç©ºé—´ç»´åº¦ï¼Ÿ

**A**: é€šè¿‡é…ç½®å‚æ•°è°ƒæ•´ï¼š
```java
config.setLatentSize(64);  // å¢åŠ åˆ°64ç»´
```

### Q4: æ§åˆ¶å™¨å¯ä»¥ä½¿ç”¨å…¶ä»–ä¼˜åŒ–æ–¹æ³•å—ï¼Ÿ

**A**: å¯ä»¥ã€‚é™¤äº†CMA-ESï¼Œè¿˜å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ã€PPOç­‰æ–¹æ³•è®­ç»ƒæ§åˆ¶å™¨ã€‚

## ğŸ”— å‚è€ƒèµ„æ–™

- [World Modelsè®ºæ–‡](https://worldmodels.github.io/) - David Ha & JÃ¼rgen Schmidhuber
- [VAEåŸç†](https://arxiv.org/abs/1312.6114) - Kingma & Welling
- [MDNåŸç†](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf) - Bishop, 1994

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-10-18)
- âœ… å®ç°å®Œæ•´çš„ä¸–ç•Œæ¨¡å‹æ¶æ„
- âœ… VAEç¼–ç å™¨ã€MDN-RNNã€æ§åˆ¶å™¨
- âœ… æƒ³è±¡è®­ç»ƒåŠŸèƒ½
- âœ… ç®€å•é©¾é©¶ç¯å¢ƒ
- âœ… æ¼”ç¤ºç¨‹åºå’Œæ–‡æ¡£

---

**TinyAI ä¸–ç•Œæ¨¡å‹** - è®©AIåœ¨æƒ³è±¡ä¸­å­¦ä¹ ! ğŸ’­ğŸ§ 
