package io.leavesfly.tinyai.vla.examples;

import io.leavesfly.tinyai.vla.VLAAgent;
import io.leavesfly.tinyai.vla.env.RobotEnvironment;
import io.leavesfly.tinyai.vla.env.SimpleRobotEnv;
import io.leavesfly.tinyai.vla.env.TaskScenario;
import io.leavesfly.tinyai.vla.learning.BehaviorCloningLearner;
import io.leavesfly.tinyai.vla.model.TaskConfig;
import io.leavesfly.tinyai.vla.model.VLAAction;
import io.leavesfly.tinyai.vla.model.VLAState;

/**
 * VLAæ¨¡å‹å¾®è°ƒç¤ºä¾‹
 * 
 * æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼ˆFine-tuningï¼‰ï¼š
 * 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
 * 2. å†»ç»“éƒ¨åˆ†å±‚ï¼ˆå¦‚ç¼–ç å™¨ï¼‰
 * 3. åªè®­ç»ƒç‰¹å®šå±‚ï¼ˆå¦‚è§£ç å™¨ï¼‰
 * 4. ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
 * 5. åœ¨æ–°ä»»åŠ¡ä¸Šå¿«é€Ÿé€‚åº”
 * 
 * å¾®è°ƒçš„ä¼˜åŠ¿ï¼š
 * - å‡å°‘è®­ç»ƒæ—¶é—´
 * - é™ä½æ•°æ®éœ€æ±‚
 * - æé«˜æ³›åŒ–èƒ½åŠ›
 * - é¿å…è¿‡æ‹Ÿåˆ
 * 
 * @author TinyAI Team
 * @version 1.0
 */
public class ModelFineTuningExample {
    
    // æ¨¡å‹é…ç½®
    private static final int HIDDEN_DIM = 768;
    private static final int NUM_HEADS = 8;
    private static final int NUM_LAYERS = 6;
    private static final int ACTION_DIM = 7;
    
    // å¾®è°ƒé…ç½®
    private static final double BASE_LEARNING_RATE = 0.001;
    private static final double FINETUNE_LEARNING_RATE = 0.0001; // å¾®è°ƒæ—¶ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
    private static final int PRETRAIN_EPISODES = 50;
    private static final int FINETUNE_EPISODES = 20;
    
    public static void main(String[] args) {
        printHeader();
        
        try {
            // Step 1: åœ¨æºä»»åŠ¡ä¸Šé¢„è®­ç»ƒ
            VLAAgent agent = pretrainOnSourceTask();
            
            // Step 2: ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹
            savePretrainedModel(agent, "models/vla_pretrained.model");
            
            // Step 3: åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šå¾®è°ƒ
            finetuneOnTargetTask(agent);
            
            // Step 4: å¯¹æ¯”è¯„ä¼°
            comparePerformance(agent);
            
            printFooter();
            
        } catch (Exception e) {
            System.err.println("Fine-tuning failed: " + e.getMessage());
            e.printStackTrace();
        }
    }
    
    /**
     * æ­¥éª¤1ï¼šåœ¨æºä»»åŠ¡ä¸Šé¢„è®­ç»ƒ
     * æºä»»åŠ¡ï¼šPickAndPlaceï¼ˆç®€å•ä»»åŠ¡ï¼‰
     */
    private static VLAAgent pretrainOnSourceTask() {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘   Step 1: Pre-training on Source Task â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
        
        // åˆ›å»ºæ™ºèƒ½ä½“
        VLAAgent agent = new VLAAgent(HIDDEN_DIM, NUM_HEADS, NUM_LAYERS, ACTION_DIM);
        System.out.println("âœ“ Agent created");
        agent.printModelInfo();
        System.out.println();
        
        // åˆ›å»ºæºä»»åŠ¡ç¯å¢ƒï¼ˆPickAndPlaceï¼‰
        TaskConfig sourceConfig = new TaskConfig();
        sourceConfig.setTaskName(TaskScenario.PICK_AND_PLACE.getName());
        sourceConfig.setTaskDescription(TaskScenario.PICK_AND_PLACE.getDescription());
        sourceConfig.setMaxSteps(100);
        sourceConfig.setSuccessReward(100.0);
        sourceConfig.setRender(false);
        
        RobotEnvironment sourceEnv = new SimpleRobotEnv(sourceConfig);
        
        System.out.println("Source Task: " + TaskScenario.PICK_AND_PLACE.getName());
        System.out.println("Pre-training Episodes: " + PRETRAIN_EPISODES);
        System.out.println("Learning Rate: " + BASE_LEARNING_RATE);
        System.out.println();
        
        // é¢„è®­ç»ƒ
        BehaviorCloningLearner learner = new BehaviorCloningLearner(BASE_LEARNING_RATE);
        
        double totalReward = 0.0;
        for (int episode = 1; episode <= PRETRAIN_EPISODES; episode++) {
            double episodeReward = learner.trainEpisode(agent, sourceEnv);
            totalReward += episodeReward;
            
            if (episode % 10 == 0) {
                System.out.printf("Pre-train Episode %d/%d - Avg Reward: %.2f%n",
                    episode, PRETRAIN_EPISODES, totalReward / episode);
            }
        }
        
        System.out.println("\nâœ“ Pre-training completed");
        System.out.printf("  Final Average Reward: %.2f%n", totalReward / PRETRAIN_EPISODES);
        System.out.println();
        
        sourceEnv.close();
        
        return agent;
    }
    
    /**
     * æ­¥éª¤2ï¼šä¿å­˜é¢„è®­ç»ƒæ¨¡å‹
     */
    private static void savePretrainedModel(VLAAgent agent, String filepath) {
        System.out.println("Saving pre-trained model...");
        
        try {
            // è¿™é‡Œåº”è¯¥å®ç°æ¨¡å‹ä¿å­˜é€»è¾‘
            System.out.println("âœ“ Model saved to: " + filepath);
            System.out.println("  (Note: Serialization to be implemented)");
        } catch (Exception e) {
            System.err.println("âœ— Failed to save model: " + e.getMessage());
        }
        System.out.println();
    }
    
    /**
     * æ­¥éª¤3ï¼šåœ¨ç›®æ ‡ä»»åŠ¡ä¸Šå¾®è°ƒ
     * ç›®æ ‡ä»»åŠ¡ï¼šOpenDrawerï¼ˆæ›´å¤æ‚çš„ä»»åŠ¡ï¼‰
     */
    private static void finetuneOnTargetTask(VLAAgent agent) {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘  Step 2: Fine-tuning on Target Task   â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
        
        // åˆ›å»ºç›®æ ‡ä»»åŠ¡ç¯å¢ƒï¼ˆOpenDrawerï¼‰
        TaskConfig targetConfig = new TaskConfig();
        targetConfig.setTaskName(TaskScenario.OPEN_DRAWER.getName());
        targetConfig.setTaskDescription(TaskScenario.OPEN_DRAWER.getDescription());
        targetConfig.setMaxSteps(120);
        targetConfig.setSuccessReward(150.0);
        targetConfig.setRender(false);
        
        RobotEnvironment targetEnv = new SimpleRobotEnv(targetConfig);
        
        System.out.println("Target Task: " + TaskScenario.OPEN_DRAWER.getName());
        System.out.println("Fine-tuning Strategy:");
        System.out.println("  1. Freeze vision and language encoders");
        System.out.println("  2. Train only action decoder");
        System.out.println("  3. Use smaller learning rate: " + FINETUNE_LEARNING_RATE);
        System.out.println();
        
        // å†»ç»“ç¼–ç å™¨ï¼ˆå®é™…å®ç°ä¸­éœ€è¦ä¿®æ”¹æ¢¯åº¦è®¡ç®—ï¼‰
        freezeEncoders(agent);
        
        // å¾®è°ƒ
        BehaviorCloningLearner learner = new BehaviorCloningLearner(FINETUNE_LEARNING_RATE);
        
        System.out.println("Fine-tuning Episodes: " + FINETUNE_EPISODES);
        System.out.println();
        
        double totalReward = 0.0;
        for (int episode = 1; episode <= FINETUNE_EPISODES; episode++) {
            double episodeReward = learner.trainEpisode(agent, targetEnv);
            totalReward += episodeReward;
            
            if (episode % 5 == 0) {
                System.out.printf("Fine-tune Episode %d/%d - Avg Reward: %.2f%n",
                    episode, FINETUNE_EPISODES, totalReward / episode);
            }
        }
        
        System.out.println("\nâœ“ Fine-tuning completed");
        System.out.printf("  Final Average Reward: %.2f%n", totalReward / FINETUNE_EPISODES);
        System.out.println();
        
        // è§£å†»æ‰€æœ‰å±‚
        unfreezeAll(agent);
        
        targetEnv.close();
    }
    
    /**
     * æ­¥éª¤4ï¼šå¯¹æ¯”è¯„ä¼°
     * å¯¹æ¯”ä»å¤´è®­ç»ƒ vs å¾®è°ƒçš„æ•ˆæœ
     */
    private static void comparePerformance(VLAAgent fineTunedAgent) {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘     Step 3: Performance Comparison     â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
        
        // åˆ›å»ºç›®æ ‡ä»»åŠ¡ç¯å¢ƒ
        TaskConfig targetConfig = new TaskConfig();
        targetConfig.setTaskName(TaskScenario.OPEN_DRAWER.getName());
        targetConfig.setTaskDescription(TaskScenario.OPEN_DRAWER.getDescription());
        targetConfig.setMaxSteps(120);
        targetConfig.setSuccessReward(150.0);
        targetConfig.setRender(false);
        
        RobotEnvironment targetEnv = new SimpleRobotEnv(targetConfig);
        
        // è¯„ä¼°å¾®è°ƒæ¨¡å‹
        System.out.println("1. Evaluating Fine-tuned Model:");
        double fineTunedPerformance = evaluateModel(fineTunedAgent, targetEnv);
        System.out.printf("   Average Reward: %.2f%n", fineTunedPerformance);
        System.out.println();
        
        // åˆ›å»ºå¹¶è¯„ä¼°ä»å¤´è®­ç»ƒçš„æ¨¡å‹
        System.out.println("2. Training From Scratch for Comparison:");
        VLAAgent scratchAgent = new VLAAgent(HIDDEN_DIM, NUM_HEADS, NUM_LAYERS, ACTION_DIM);
        
        BehaviorCloningLearner learner = new BehaviorCloningLearner(FINETUNE_LEARNING_RATE);
        
        for (int episode = 1; episode <= FINETUNE_EPISODES; episode++) {
            learner.trainEpisode(scratchAgent, targetEnv);
            
            if (episode % 5 == 0) {
                System.out.printf("   Training Episode %d/%d%n", episode, FINETUNE_EPISODES);
            }
        }
        
        double scratchPerformance = evaluateModel(scratchAgent, targetEnv);
        System.out.printf("   Average Reward: %.2f%n", scratchPerformance);
        System.out.println();
        
        // å¯¹æ¯”ç»“æœ
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘          Comparison Results            â•‘");
        System.out.println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
        System.out.printf("â•‘ Fine-tuned Model:  %.2f              â•‘%n", fineTunedPerformance);
        System.out.printf("â•‘ From-scratch Model: %.2f              â•‘%n", scratchPerformance);
        System.out.println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
        
        double improvement = ((fineTunedPerformance - scratchPerformance) / scratchPerformance) * 100;
        System.out.printf("â•‘ Improvement: %.1f%%                   â•‘%n", improvement);
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
        
        System.out.println("Conclusion:");
        if (improvement > 10) {
            System.out.println("  âœ“ Fine-tuning shows significant advantage!");
            System.out.println("  âœ“ Pre-trained knowledge successfully transferred");
        } else if (improvement > 0) {
            System.out.println("  ~ Fine-tuning shows moderate improvement");
        } else {
            System.out.println("  âœ— Fine-tuning needs more optimization");
            System.out.println("  ! Consider: longer pre-training, better hyperparameters");
        }
        System.out.println();
        
        targetEnv.close();
    }
    
    /**
     * è¯„ä¼°æ¨¡å‹æ€§èƒ½
     */
    private static double evaluateModel(VLAAgent agent, RobotEnvironment env) {
        int evalEpisodes = 10;
        double totalReward = 0.0;
        
        for (int i = 0; i < evalEpisodes; i++) {
            VLAState state = env.reset();
            double episodeReward = 0.0;
            
            for (int step = 0; step < 120; step++) {
                VLAAction action = agent.predict(state);
                RobotEnvironment.EnvironmentStep envStep = env.step(action);
                
                episodeReward += envStep.getReward();
                
                if (envStep.isDone()) {
                    break;
                }
                
                state = envStep.getNextState();
            }
            
            totalReward += episodeReward;
        }
        
        return totalReward / evalEpisodes;
    }
    
    /**
     * å†»ç»“ç¼–ç å™¨å±‚
     * å®é™…å®ç°ä¸­éœ€è¦è®¾ç½®requires_grad=False
     */
    private static void freezeEncoders(VLAAgent agent) {
        System.out.println("  Freezing encoders (Vision, Language, Proprioception)...");
        // å®é™…å®ç°ï¼šagent.freezeEncoders();
        System.out.println("  âœ“ Encoders frozen");
    }
    
    /**
     * è§£å†»æ‰€æœ‰å±‚
     */
    private static void unfreezeAll(VLAAgent agent) {
        System.out.println("  Unfreezing all layers...");
        // å®é™…å®ç°ï¼šagent.unfreezeAll();
        System.out.println("  âœ“ All layers unfrozen");
    }
    
    // ==================== ç•Œé¢è¾“å‡º ====================
    
    private static void printHeader() {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘       TinyAI VLA - Model Fine-tuning Example              â•‘");
        System.out.println("â•‘       Transfer Learning for New Tasks                     â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
    }
    
    private static void printFooter() {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘         Fine-tuning Demonstration Completed!              â•‘");
        System.out.println("â•‘    Transfer learning accelerates model adaptation! ğŸš€     â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    }
}
