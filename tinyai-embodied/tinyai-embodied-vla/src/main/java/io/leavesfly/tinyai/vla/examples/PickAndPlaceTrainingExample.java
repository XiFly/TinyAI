package io.leavesfly.tinyai.vla.examples;

import io.leavesfly.tinyai.vla.VLAAgent;
import io.leavesfly.tinyai.vla.env.RobotEnvironment;
import io.leavesfly.tinyai.vla.env.SimpleRobotEnv;
import io.leavesfly.tinyai.vla.env.TaskScenario;
import io.leavesfly.tinyai.vla.learning.BehaviorCloningLearner;
import io.leavesfly.tinyai.vla.learning.VLALearningEngine;
import io.leavesfly.tinyai.vla.model.TaskConfig;
import io.leavesfly.tinyai.vla.model.VLAAction;
import io.leavesfly.tinyai.vla.model.VLAState;

import java.util.ArrayList;
import java.util.List;

/**
 * PickAndPlaceä»»åŠ¡å®Œæ•´è®­ç»ƒç¤ºä¾‹
 * 
 * æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨VLAæ™ºèƒ½ä½“å®Œæˆ"æ‹¾å–å¹¶æ”¾ç½®"ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼š
 * 1. ç¯å¢ƒåˆå§‹åŒ–
 * 2. æ™ºèƒ½ä½“åˆ›å»ºä¸é…ç½®
 * 3. ä¸“å®¶æ¼”ç¤ºæ•°æ®æ”¶é›†
 * 4. è¡Œä¸ºå…‹éš†è®­ç»ƒ
 * 5. æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–
 * 6. æ¨¡å‹ä¿å­˜ä¸åŠ è½½
 * 
 * @author TinyAI Team
 * @version 1.0
 */
public class PickAndPlaceTrainingExample {
    
    // è®­ç»ƒè¶…å‚æ•°
    private static final int HIDDEN_DIM = 768;
    private static final int NUM_HEADS = 8;
    private static final int NUM_LAYERS = 6;
    private static final int ACTION_DIM = 7;
    
    private static final double LEARNING_RATE = 0.001;
    private static final int TRAINING_EPISODES = 100;
    private static final int EVALUATION_EPISODES = 10;
    private static final int MAX_STEPS_PER_EPISODE = 100;
    
    public static void main(String[] args) {
        printHeader();
        
        try {
            // Step 1: ç¯å¢ƒåˆå§‹åŒ–
            RobotEnvironment env = createEnvironment();
            
            // Step 2: åˆ›å»ºVLAæ™ºèƒ½ä½“
            VLAAgent agent = createAgent();
            
            // Step 3: æ”¶é›†ä¸“å®¶æ¼”ç¤ºæ•°æ®ï¼ˆå¯é€‰ï¼‰
            List<DemonstrationData> demonstrations = collectDemonstrations(env, 10);
            System.out.println("Collected " + demonstrations.size() + " expert demonstrations\n");
            
            // Step 4: è¡Œä¸ºå…‹éš†è®­ç»ƒ
            VLALearningEngine learner = trainAgent(agent, env, demonstrations);
            
            // Step 5: è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹
            evaluateAgent(agent, env);
            
            // Step 6: å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
            visualizeTraining(learner);
            
            // Step 7: ä¿å­˜æ¨¡å‹
            saveModel(agent, "models/vla_pick_and_place.model");
            
            // Step 8: æ¸…ç†èµ„æº
            env.close();
            
            printFooter();
            
        } catch (Exception e) {
            System.err.println("Training failed: " + e.getMessage());
            e.printStackTrace();
        }
    }
    
    /**
     * åˆ›å»ºPickAndPlaceä»»åŠ¡ç¯å¢ƒ
     */
    private static RobotEnvironment createEnvironment() {
        System.out.println("=== Step 1: Creating Environment ===");
        
        TaskConfig taskConfig = new TaskConfig();
        taskConfig.setTaskName(TaskScenario.PICK_AND_PLACE.getName());
        taskConfig.setTaskDescription(TaskScenario.PICK_AND_PLACE.getDescription());
        taskConfig.setMaxSteps(MAX_STEPS_PER_EPISODE);
        taskConfig.setSuccessReward(100.0);
        taskConfig.setStepPenalty(-0.1);
        taskConfig.setRender(false);
        
        RobotEnvironment env = new SimpleRobotEnv(taskConfig);
        
        System.out.println("âœ“ Environment created successfully");
        System.out.println("  Task: " + TaskScenario.PICK_AND_PLACE.getName());
        System.out.println("  Difficulty: " + TaskScenario.PICK_AND_PLACE.getDifficultyStars());
        System.out.println("  Max Steps: " + MAX_STEPS_PER_EPISODE);
        System.out.println();
        
        return env;
    }
    
    /**
     * åˆ›å»ºVLAæ™ºèƒ½ä½“
     */
    private static VLAAgent createAgent() {
        System.out.println("=== Step 2: Creating VLA Agent ===");
        
        VLAAgent agent = new VLAAgent(HIDDEN_DIM, NUM_HEADS, NUM_LAYERS, ACTION_DIM);
        
        System.out.println("âœ“ Agent created successfully");
        agent.printModelInfo();
        System.out.println();
        
        return agent;
    }
    
    /**
     * æ”¶é›†ä¸“å®¶æ¼”ç¤ºæ•°æ®
     * åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›æ•°æ®å¯èƒ½æ¥è‡ªï¼š
     * 1. äººç±»è¿œç¨‹æ“ä½œ
     * 2. é¢„å®šä¹‰çš„æ§åˆ¶ç­–ç•¥
     * 3. å·²è®­ç»ƒå¥½çš„æ¨¡å‹
     */
    private static List<DemonstrationData> collectDemonstrations(
            RobotEnvironment env, int numDemonstrations) {
        System.out.println("=== Step 3: Collecting Expert Demonstrations ===");
        
        List<DemonstrationData> demonstrations = new ArrayList<>();
        
        for (int i = 0; i < numDemonstrations; i++) {
            System.out.printf("Collecting demonstration %d/%d...%n", i + 1, numDemonstrations);
            
            // ä½¿ç”¨ç®€å•çš„å¯å‘å¼ç­–ç•¥ç”Ÿæˆæ¼”ç¤ºæ•°æ®
            VLAState state = env.reset();
            List<StateActionPair> trajectory = new ArrayList<>();
            
            int step = 0;
            while (step < MAX_STEPS_PER_EPISODE) {
                // è¿™é‡Œä½¿ç”¨ç®€å•çš„å¯å‘å¼ç­–ç•¥ï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ç”¨ä¸“å®¶ç­–ç•¥ï¼‰
                VLAAction action = getExpertAction(state, env);
                
                trajectory.add(new StateActionPair(state, action));
                
                RobotEnvironment.EnvironmentStep envStep = env.step(action);
                
                if (envStep.isDone()) {
                    break;
                }
                
                state = envStep.getNextState();
                step++;
            }
            
            demonstrations.add(new DemonstrationData(trajectory));
        }
        
        System.out.println("âœ“ Demonstrations collected\n");
        return demonstrations;
    }
    
    /**
     * è®­ç»ƒæ™ºèƒ½ä½“
     */
    private static VLALearningEngine trainAgent(
            VLAAgent agent, 
            RobotEnvironment env,
            List<DemonstrationData> demonstrations) {
        System.out.println("=== Step 4: Training Agent (Behavior Cloning) ===");
        
        VLALearningEngine learner = new BehaviorCloningLearner(LEARNING_RATE);
        
        // å¦‚æœæœ‰æ¼”ç¤ºæ•°æ®ï¼Œå…ˆè¿›è¡Œé¢„è®­ç»ƒ
        if (demonstrations != null && !demonstrations.isEmpty()) {
            System.out.println("Pre-training from demonstrations...");
            learner.pretrainFromDemonstrations(agent, demonstrations);
            System.out.println("âœ“ Pre-training completed\n");
        }
        
        // åœ¨çº¿è®­ç»ƒ
        System.out.println("Starting online training...");
        System.out.println("Training Episodes: " + TRAINING_EPISODES);
        System.out.println("Learning Rate: " + LEARNING_RATE);
        System.out.println();
        
        // è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•æŒ‡æ ‡
        for (int episode = 1; episode <= TRAINING_EPISODES; episode++) {
            double episodeReward = learner.trainEpisode(agent, env);
            
            if (episode % 10 == 0) {
                System.out.printf("Episode %d/%d - Reward: %.2f%n", 
                    episode, TRAINING_EPISODES, episodeReward);
            }
        }
        
        System.out.println("\nâœ“ Training completed\n");
        return learner;
    }
    
    /**
     * è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½
     */
    private static void evaluateAgent(VLAAgent agent, RobotEnvironment env) {
        System.out.println("=== Step 5: Evaluating Agent ===");
        
        double totalReward = 0.0;
        int successCount = 0;
        List<Double> episodeRewards = new ArrayList<>();
        
        for (int i = 0; i < EVALUATION_EPISODES; i++) {
            VLAState state = env.reset();
            double episodeReward = 0.0;
            int step = 0;
            
            while (step < MAX_STEPS_PER_EPISODE) {
                VLAAction action = agent.predict(state);
                RobotEnvironment.EnvironmentStep envStep = env.step(action);
                
                episodeReward += envStep.getReward();
                
                if (envStep.isDone()) {
                    if (episodeReward > 80.0) { // æˆåŠŸé˜ˆå€¼
                        successCount++;
                    }
                    break;
                }
                
                state = envStep.getNextState();
                step++;
            }
            
            episodeRewards.add(episodeReward);
            totalReward += episodeReward;
        }
        
        double avgReward = totalReward / EVALUATION_EPISODES;
        double successRate = (double) successCount / EVALUATION_EPISODES * 100;
        
        System.out.println("Evaluation Results:");
        System.out.printf("  Average Reward: %.2f%n", avgReward);
        System.out.printf("  Success Rate: %.1f%%%n", successRate);
        System.out.printf("  Min Reward: %.2f%n", episodeRewards.stream().min(Double::compare).orElse(0.0));
        System.out.printf("  Max Reward: %.2f%n", episodeRewards.stream().max(Double::compare).orElse(0.0));
        System.out.println();
    }
    
    /**
     * å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
     */
    private static void visualizeTraining(VLALearningEngine learner) {
        System.out.println("=== Step 6: Visualizing Training ===");
        
        // æ‰“å°è®­ç»ƒæ›²çº¿ï¼ˆç®€åŒ–ç‰ˆï¼‰
        System.out.println("Training Metrics:");
        System.out.println("  Total Training Episodes: " + TRAINING_EPISODES);
        System.out.println("  Learning Rate: " + LEARNING_RATE);
        System.out.println("  Note: Detailed metrics can be exported to TensorBoard");
        System.out.println();
    }
    
    /**
     * ä¿å­˜æ¨¡å‹
     */
    private static void saveModel(VLAAgent agent, String filepath) {
        System.out.println("=== Step 7: Saving Model ===");
        
        try {
            // è¿™é‡Œåº”è¯¥å®ç°æ¨¡å‹ä¿å­˜é€»è¾‘
            System.out.println("âœ“ Model saved to: " + filepath);
            System.out.println("  (Note: Model serialization to be implemented)");
        } catch (Exception e) {
            System.err.println("âœ— Failed to save model: " + e.getMessage());
        }
        System.out.println();
    }
    
    /**
     * è·å–ä¸“å®¶åŠ¨ä½œï¼ˆå¯å‘å¼ç­–ç•¥ï¼‰
     * è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹ï¼Œå®é™…åº”è¯¥ä½¿ç”¨çœŸå®çš„ä¸“å®¶ç­–ç•¥
     */
    private static VLAAction getExpertAction(VLAState state, RobotEnvironment env) {
        // ç®€åŒ–çš„å¯å‘å¼ç­–ç•¥ï¼š
        // 1. å¦‚æœæœªæŠ“å–ç‰©ä½“ï¼Œç§»åŠ¨åˆ°ç‰©ä½“ä½ç½®å¹¶æŠ“å–
        // 2. å¦‚æœå·²æŠ“å–ç‰©ä½“ï¼Œç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®å¹¶é‡Šæ”¾
        
        // è¿™é‡Œè¿”å›ä¸€ä¸ªéšæœºåŠ¨ä½œä½œä¸ºå ä½ç¬¦
        return env.sampleAction();
    }
    
    // ==================== è¾…åŠ©æ•°æ®ç»“æ„ ====================
    
    /**
     * çŠ¶æ€-åŠ¨ä½œå¯¹
     */
    static class StateActionPair {
        private final VLAState state;
        private final VLAAction action;
        
        public StateActionPair(VLAState state, VLAAction action) {
            this.state = state;
            this.action = action;
        }
        
        public VLAState getState() { return state; }
        public VLAAction getAction() { return action; }
    }
    
    /**
     * æ¼”ç¤ºæ•°æ®
     */
    static class DemonstrationData {
        private final List<StateActionPair> trajectory;
        
        public DemonstrationData(List<StateActionPair> trajectory) {
            this.trajectory = trajectory;
        }
        
        public List<StateActionPair> getTrajectory() { return trajectory; }
    }
    
    // ==================== ç•Œé¢è¾“å‡º ====================
    
    private static void printHeader() {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘       TinyAI VLA - PickAndPlace Training Example          â•‘");
        System.out.println("â•‘       Vision-Language-Action Embodied Intelligence        â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        System.out.println();
    }
    
    private static void printFooter() {
        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘              Training Completed Successfully!             â•‘");
        System.out.println("â•‘                  Happy Robot Learning! ğŸ¤–                  â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    }
}
