# TinyAI VLA 模块使用指南

## 快速开始

### 1. 环境要求

- **JDK版本**: 17 或更高
- **Maven版本**: 3.6+
- **内存要求**: 推荐8GB以上

### 2. 安装模块

```bash
cd tinyai-agent-embodied-vla
mvn clean install
```

### 3. 运行演示程序

```bash
mvn exec:java -Dexec.mainClass="io.leavesfly.tinyai.vla.VLADemo"
```

## 基本使用

### 创建VLA智能体

```java
// 创建VLA智能体
VLAAgent agent = new VLAAgent(
    768,   // 隐藏层维度
    8,     // 注意力头数
    6,     // Transformer层数
    7      // 动作维度
);

// 打印模型信息
agent.printModelInfo();
```

### 准备输入数据

```java
// 1. 创建视觉输入
double[][][] rgbImage = new double[64][64][3];
// ... 填充图像数据
VisionInput visionInput = new VisionInput(new NdArray(rgbImage));

// 2. 创建语言输入
LanguageInput languageInput = new LanguageInput(
    "Pick up the red cube and place it in the box"
);

// 3. 创建本体感知输入
double[] jointPositions = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
double[] jointVelocities = {0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};

ProprioceptionInput proprioInput = new ProprioceptionInput(
    new NdArray(jointPositions),
    new NdArray(jointVelocities)
);

// 4. 创建VLA状态
VLAState state = new VLAState(visionInput, languageInput, proprioInput);
```

### 预测动作

```java
// 预测动作
VLAAction action = agent.predict(state);

// 获取动作信息
NdArray continuousAction = action.getContinuousAction();
int discreteAction = action.getDiscreteAction();
ActionType actionType = action.getActionType();
double confidence = action.getConfidence();
String feedback = action.getLanguageFeedback();

System.out.println("Action Type: " + actionType.getDescription());
System.out.println("Confidence: " + confidence);
System.out.println("Feedback: " + feedback);
```

## 环境交互

### 创建环境

```java
// 创建任务配置
TaskConfig config = new TaskConfig();
config.setTaskName("PickAndPlace");
config.setMaxSteps(100);
config.setSuccessReward(100.0);

// 创建环境
RobotEnvironment env = new SimpleRobotEnv(config);
```

### 运行Episode

```java
// 重置环境
VLAState state = env.reset();
double totalReward = 0.0;
int step = 0;

while (true) {
    // 智能体预测动作
    VLAAction action = agent.predict(state);
    
    // 执行动作
    RobotEnvironment.EnvironmentStep envStep = env.step(action);
    
    totalReward += envStep.getReward();
    
    // 检查终止
    if (envStep.isDone()) {
        break;
    }
    
    state = envStep.getNextState();
    step++;
}

System.out.println("Total Steps: " + step);
System.out.println("Total Reward: " + totalReward);

// 清理资源
env.close();
```

## 训练智能体

### 行为克隆训练

```java
// 创建学习引擎
VLALearningEngine learner = new BehaviorCloningLearner(0.001);

// 训练智能体
learner.train(agent, env, 100); // 100个回合

// 评估性能
double avgReward = learner.evaluate(agent, env, 10);
System.out.println("Average Reward: " + avgReward);

// 保存检查点
learner.saveCheckpoint("models/vla_checkpoint.pt");
```

## 高级用法

### 自定义任务环境

```java
public class CustomRobotEnv implements RobotEnvironment {
    
    @Override
    public VLAState reset() {
        // 初始化环境状态
        return createInitialState();
    }
    
    @Override
    public EnvironmentStep step(VLAAction action) {
        // 执行动作逻辑
        VLAState nextState = applyAction(action);
        double reward = calculateReward(nextState);
        boolean done = checkTermination(nextState);
        
        Map<String, Object> info = new HashMap<>();
        // 添加额外信息
        
        return new EnvironmentStep(nextState, reward, done, info);
    }
    
    private double calculateReward(VLAState state) {
        // 自定义奖励函数
        return 0.0;
    }
    
    // 实现其他接口方法
}
```

### 多任务场景切换

```java
// 定义不同任务场景
TaskConfig pickAndPlaceConfig = new TaskConfig();
pickAndPlaceConfig.setTaskName(TaskScenario.PICK_AND_PLACE.getName());

TaskConfig stackBlocksConfig = new TaskConfig();
stackBlocksConfig.setTaskName(TaskScenario.STACK_BLOCKS.getName());

// 切换任务
RobotEnvironment env1 = new SimpleRobotEnv(pickAndPlaceConfig);
RobotEnvironment env2 = new SimpleRobotEnv(stackBlocksConfig);

// 在不同任务上训练/评估
learner.train(agent, env1, 50);
learner.train(agent, env2, 50);
```

### 图像预处理

```java
// 归一化图像
NdArray normalizedImage = ImageProcessor.normalize(rawImage);

// 调整大小
NdArray resizedImage = ImageProcessor.resize(rawImage, 64, 64);

// 标准化
double[] mean = {0.485, 0.456, 0.406};
double[] std = {0.229, 0.224, 0.225};
NdArray standardizedImage = ImageProcessor.standardize(rawImage, mean, std);

// 数据增强
NdArray augmentedImage = ImageProcessor.randomHorizontalFlip(rawImage, 0.5);
NdArray noisyImage = ImageProcessor.addNoise(rawImage, 0.01);
```

### 动作归一化

```java
// 定义动作空间范围
double[] actionLow = {-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0};
double[] actionHigh = {1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0};

ActionNormalizer normalizer = new ActionNormalizer(actionLow, actionHigh);

// 归一化动作
NdArray normalizedAction = normalizer.normalize(rawAction);

// 反归一化动作
NdArray denormalizedAction = normalizer.denormalize(normalizedAction);
```

## 调试与监控

### 打印状态信息

```java
// 打印VLA状态
System.out.println(state.toString());

// 打印动作信息
System.out.println(action.toString());

// 打印环境信息
System.out.println("Action Space: " + env.getActionSpace());
System.out.println("Observation Space: " + env.getObservationSpace());
```

### 监控训练过程

```java
for (int episode = 0; episode < numEpisodes; episode++) {
    VLAState state = env.reset();
    double episodeReward = 0.0;
    int steps = 0;
    
    while (!done) {
        VLAAction action = agent.predict(state);
        EnvironmentStep step = env.step(action);
        
        episodeReward += step.getReward();
        steps++;
        
        state = step.getNextState();
    }
    
    // 每10个回合打印一次
    if (episode % 10 == 0) {
        System.out.printf("Episode %d: Reward=%.2f, Steps=%d%n", 
                         episode, episodeReward, steps);
    }
}
```

## 性能优化

### 批处理推理

```java
// 收集多个状态
List<VLAState> stateBatch = new ArrayList<>();
// ... 填充状态

// 批量预测（未来支持）
for (VLAState state : stateBatch) {
    VLAAction action = agent.predict(state);
    // 处理动作
}
```

### 内存管理

```java
// 及时清理资源
env.close();

// 避免内存泄漏
state = null;
action = null;
System.gc(); // 手动触发垃圾回收（谨慎使用）
```

## 常见问题

### Q1: 如何调整模型大小？

```java
// 减小模型：减少隐藏维度和层数
VLAAgent smallAgent = new VLAAgent(256, 4, 3, 7);

// 增大模型：增加隐藏维度和层数
VLAAgent largeAgent = new VLAAgent(1024, 16, 12, 7);
```

### Q2: 如何提高训练速度？

- 减少环境最大步数
- 使用更少的Transformer层
- 简化奖励函数计算
- 批量处理数据（未来支持）

### Q3: 如何评估模型性能？

```java
// 在多个回合上评估
double avgReward = learner.evaluate(agent, env, 100);

// 计算成功率
int successCount = 0;
for (int i = 0; i < 100; i++) {
    // 运行一个回合
    if (isTaskSuccessful()) {
        successCount++;
    }
}
double successRate = successCount / 100.0;
System.out.println("Success Rate: " + successRate);
```

### Q4: 如何处理不同的语言指令？

```java
// 支持中英文指令
LanguageInput langInput1 = new LanguageInput("Pick up the red cube");
LanguageInput langInput2 = new LanguageInput("拿起红色方块");

// Tokenizer会自动处理
```

## 示例代码

完整示例代码请参考：
- `VLADemo.java` - 基础演示
- `VLAAgentTest.java` - 单元测试
- `VLAModelTest.java` - 数据模型测试

## 参考资料

- [技术架构文档](技术架构文档.md)
- [API参考文档](API参考.md)
- [TinyAI项目主页](../../README.md)

---

**版本**: v1.0  
**更新时间**: 2025-10-18  
**作者**: TinyAI 开发团队
