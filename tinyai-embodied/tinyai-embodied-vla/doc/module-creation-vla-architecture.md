# TinyAI è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰å…·èº«æ™ºèƒ½æ¨¡å—è®¾è®¡æ–‡æ¡£

## 1. æ¦‚è¿°

### 1.1 æ¨¡å—ç®€ä»‹

`tinyai-agent-embodied-vla` æ˜¯ TinyAI æ™ºèƒ½ä½“ç³»ç»Ÿå±‚çš„é«˜çº§å…·èº«æ™ºèƒ½æ¨¡å—ï¼Œä¸“æ³¨äºå®ç°åŸºäº**è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVision-Language-Action, VLAï¼‰æ¶æ„**çš„ç«¯åˆ°ç«¯å…·èº«æ™ºèƒ½ç³»ç»Ÿã€‚è¯¥æ¨¡å—é€šè¿‡èåˆè§†è§‰æ„ŸçŸ¥ã€è‡ªç„¶è¯­è¨€ç†è§£å’ŒåŠ¨ä½œç”Ÿæˆä¸‰å¤§æ ¸å¿ƒèƒ½åŠ›ï¼Œæ„å»ºèƒ½å¤Ÿç†è§£æŒ‡ä»¤ã€æ„ŸçŸ¥ç¯å¢ƒå¹¶æ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡çš„æ™ºèƒ½ä½“ã€‚

### 1.2 è®¾è®¡ç›®æ ‡

- **å¤šæ¨¡æ€èåˆ**ï¼šå®ç°è§†è§‰ã€è¯­è¨€ã€åŠ¨ä½œä¸‰ç§æ¨¡æ€çš„æ·±åº¦èåˆ
- **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šæ”¯æŒä»åŸå§‹æ„ŸçŸ¥è¾“å…¥ç›´æ¥åˆ°åŠ¨ä½œè¾“å‡ºçš„ç«¯åˆ°ç«¯å­¦ä¹ 
- **æŒ‡ä»¤ç†è§£**ï¼šèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¹¶è½¬åŒ–ä¸ºå…·ä½“åŠ¨ä½œåºåˆ—
- **åœºæ™¯é€‚åº”**ï¼šæ”¯æŒå¤šç§å…·èº«æ™ºèƒ½åœºæ™¯ï¼ˆæœºå™¨äººæ“ä½œã€å¯¼èˆªã€äº¤äº’ç­‰ï¼‰
- **æ¨¡å—å¤ç”¨**ï¼šå……åˆ†å¤ç”¨ TinyAI é¡¹ç›®å†…éƒ¨å·²æœ‰ç»„ä»¶ï¼Œå‡å°‘å¤–éƒ¨ä¾èµ–

### 1.3 æ ¸å¿ƒç‰¹æ€§

- ğŸ¯ **ç»Ÿä¸€æ¶æ„**ï¼šVLAä¸‰æ¨¡æ€ç»Ÿä¸€å»ºæ¨¡ï¼Œå…±äº«Transformeréª¨å¹²ç½‘ç»œ
- ğŸ§  **è¯­è¨€å¼•å¯¼**ï¼šè‡ªç„¶è¯­è¨€æŒ‡ä»¤å¼•å¯¼è§†è§‰æ³¨æ„åŠ›å’ŒåŠ¨ä½œç”Ÿæˆ
- ğŸ‘ï¸ **è§†è§‰ç†è§£**ï¼šæ·±åº¦å›¾åƒç‰¹å¾æå–ä¸åœºæ™¯è¯­ä¹‰ç†è§£
- ğŸ¤– **ç²¾å‡†æ§åˆ¶**ï¼šè¿ç»­åŠ¨ä½œç©ºé—´ä¸ç¦»æ•£åŠ¨ä½œç©ºé—´ç»Ÿä¸€å»ºæ¨¡
- ğŸ”„ **é—­ç¯åé¦ˆ**ï¼šæ‰§è¡Œç»“æœåé¦ˆåˆ°æ„ŸçŸ¥å±‚ï¼Œå½¢æˆå®Œæ•´é—­ç¯
- ğŸ“š **é›¶æ ·æœ¬æ³›åŒ–**ï¼šæ”¯æŒé€šè¿‡è¯­è¨€æŒ‡ä»¤å®Œæˆæœªè®­ç»ƒè¿‡çš„æ–°ä»»åŠ¡

### 1.4 ä¸ç°æœ‰æ¨¡å—çš„å…³ç³»

æœ¬æ¨¡å—åœ¨ `tinyai-agent-embodied` åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ï¼š

| å¯¹æ¯”ç»´åº¦ | tinyai-agent-embodied | tinyai-agent-embodied-vla |
|---------|----------------------|--------------------------|
| æ ¸å¿ƒæ¶æ„ | æ„ŸçŸ¥-å†³ç­–-æ‰§è¡Œåˆ†ç¦» | è§†è§‰-è¯­è¨€-åŠ¨ä½œç»Ÿä¸€èåˆ |
| è¾“å…¥æ¨¡æ€ | ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆæ•°å€¼ï¼‰ | å›¾åƒ + è‡ªç„¶è¯­è¨€æŒ‡ä»¤ |
| å†³ç­–æ–¹å¼ | ç­–ç•¥ç½‘ç»œ + è§„åˆ™çº¦æŸ | Transformerç«¯åˆ°ç«¯æ¨ç† |
| è¾“å‡ºå½¢å¼ | ç»“æ„åŒ–åŠ¨ä½œå‚æ•° | åŠ¨ä½œåºåˆ— + è‡ªç„¶è¯­è¨€åé¦ˆ |
| åº”ç”¨åœºæ™¯ | è‡ªåŠ¨é©¾é©¶ | æœºå™¨äººæ“ä½œã€äººæœºåä½œ |

## 2. æŠ€æœ¯æ¶æ„

### 2.1 ç³»ç»Ÿæ•´ä½“æ¶æ„

```mermaid
graph TB
    subgraph "è¾“å…¥å±‚"
        A1[RGBå›¾åƒè¾“å…¥]
        A2[è‡ªç„¶è¯­è¨€æŒ‡ä»¤]
        A3[æœ¬ä½“æ„ŸçŸ¥æ•°æ®]
    end
    
    subgraph "ç¼–ç å±‚"
        B1[è§†è§‰ç¼–ç å™¨<br/>Vision Encoder]
        B2[è¯­è¨€ç¼–ç å™¨<br/>Language Encoder]
        B3[æœ¬ä½“ç¼–ç å™¨<br/>Proprioception Encoder]
    end
    
    subgraph "èåˆå±‚"
        C1[è·¨æ¨¡æ€æ³¨æ„åŠ›<br/>Cross-Modal Attention]
        C2[VLA Transformeræ ¸å¿ƒ<br/>å¤šå±‚è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ]
    end
    
    subgraph "è§£ç å±‚"
        D1[åŠ¨ä½œè§£ç å™¨<br/>Action Decoder]
        D2[è¯­è¨€åé¦ˆç”Ÿæˆå™¨<br/>Language Feedback]
    end
    
    subgraph "è¾“å‡ºå±‚"
        E1[è¿ç»­åŠ¨ä½œ<br/>ä½ç½®/é€Ÿåº¦/åŠ›]
        E2[ç¦»æ•£åŠ¨ä½œ<br/>æŠ“å–/é‡Šæ”¾]
        E3[è‡ªç„¶è¯­è¨€æè¿°<br/>æ‰§è¡ŒçŠ¶æ€]
    end
    
    subgraph "ç¯å¢ƒäº¤äº’å±‚"
        F1[æœºå™¨äººç¯å¢ƒ<br/>Robot Environment]
        F2[ä»¿çœŸç¯å¢ƒ<br/>Simulation]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B2 --> C1
    B3 --> C1
    
    C1 --> C2
    C2 --> D1
    C2 --> D2
    
    D1 --> E1
    D1 --> E2
    D2 --> E3
    
    E1 --> F1
    E2 --> F1
    F1 --> F2
    F2 -.åé¦ˆ.-> A1
    F2 -.åé¦ˆ.-> A3
```

### 2.2 åˆ†å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VLAæ™ºèƒ½ä½“æ ¸å¿ƒå±‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ VLAç¼–ç å™¨     â”‚ è·¨æ¨¡æ€èåˆ    â”‚ VLAè§£ç å™¨     â”‚ å­¦ä¹ å¼•æ“   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ¨¡æ€å¤„ç†å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ è§†è§‰å¤„ç†      â”‚ è¯­è¨€å¤„ç†      â”‚ åŠ¨ä½œå¤„ç†      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ç¯å¢ƒä»¿çœŸå±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ æœºå™¨äººç¯å¢ƒ    â”‚ æ“ä½œä»»åŠ¡      â”‚ åœºæ™¯ç®¡ç†      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  TinyAI æ ¸å¿ƒæ¨¡å—å±‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ NdArray  â”‚ AutoGrad â”‚ NeuralNetâ”‚ GPTæ¨¡å‹   â”‚ RLæ¨¡å—    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 æ ¸å¿ƒæ¨¡å—èŒè´£

| æ¨¡å— | ä¸»è¦èŒè´£ | ä¾èµ–ç»„ä»¶ |
|-----|---------|---------|
| **VLAEncoder** | å¤šæ¨¡æ€è¾“å…¥ç¼–ç ä¸ç‰¹å¾æå– | GPT Transformerã€NdArray |
| **CrossModalFusion** | è§†è§‰-è¯­è¨€-æœ¬ä½“æ„ŸçŸ¥ç‰¹å¾èåˆ | Attentionæœºåˆ¶ã€Layer Norm |
| **VLADecoder** | åŠ¨ä½œåºåˆ—ç”Ÿæˆä¸è¯­è¨€åé¦ˆ | Transformer Decoderã€MLP |
| **VisionProcessor** | å›¾åƒé¢„å¤„ç†ä¸å·ç§¯ç‰¹å¾æå– | NdArrayã€å·ç§¯ç¥ç»ç½‘ç»œ |
| **LanguageProcessor** | æ–‡æœ¬ç¼–ç ä¸è¯­ä¹‰ç†è§£ | GPT Tokenizerã€Embedding |
| **ActionExecutor** | åŠ¨ä½œæ‰§è¡Œä¸ç¯å¢ƒäº¤äº’ | Robot Environment API |
| **RobotEnvironment** | æœºå™¨äººä»¿çœŸç¯å¢ƒ | ç‰©ç†å¼•æ“æŠ½è±¡ã€å¥–åŠ±å‡½æ•° |
| **VLALearningEngine** | ç«¯åˆ°ç«¯è®­ç»ƒä¸ç­–ç•¥ä¼˜åŒ– | RLæ¨¡å—ã€AutoGrad |

## 3. æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 3.1 æ•°æ®æ¨¡å‹å±‚

#### 3.1.1 å¤šæ¨¡æ€è¾“å…¥è¡¨ç¤º

**VisionInput - è§†è§‰è¾“å…¥**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç»´åº¦ |
|-----|------|------|-----|
| rgbImage | NdArray | RGBå›¾åƒ | [H, W, 3] |
| depthImage | NdArray | æ·±åº¦å›¾ï¼ˆå¯é€‰ï¼‰ | [H, W, 1] |
| imageFeatures | NdArray | è§†è§‰ç¼–ç ç‰¹å¾ | [196, 768] |
| objectMasks | NdArray | ç›®æ ‡åˆ†å‰²æ©ç  | [N, H, W] |
| timestamp | long | é‡‡é›†æ—¶é—´æˆ³ | - |

**LanguageInput - è¯­è¨€è¾“å…¥**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç»´åº¦ |
|-----|------|------|-----|
| instruction | String | è‡ªç„¶è¯­è¨€æŒ‡ä»¤ | - |
| tokenIds | NdArray | Token IDåºåˆ— | [seq_len] |
| attentionMask | NdArray | æ³¨æ„åŠ›æ©ç  | [seq_len] |
| embeddings | NdArray | æ–‡æœ¬åµŒå…¥å‘é‡ | [seq_len, 768] |

**ProprioceptionInput - æœ¬ä½“æ„ŸçŸ¥è¾“å…¥**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç»´åº¦ |
|-----|------|------|-----|
| jointPositions | NdArray | å…³èŠ‚ä½ç½® | [n_joints] |
| jointVelocities | NdArray | å…³èŠ‚é€Ÿåº¦ | [n_joints] |
| endEffectorPose | NdArray | æœ«ç«¯æ‰§è¡Œå™¨å§¿æ€ | [7] (ä½ç½®+å››å…ƒæ•°) |
| gripperState | double | å¤¹çˆªçŠ¶æ€ | [1] |

#### 3.1.2 ç»Ÿä¸€çŠ¶æ€è¡¨ç¤º

**VLAState - VLAç»Ÿä¸€çŠ¶æ€**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|-----|------|------|
| visionInput | VisionInput | è§†è§‰æ¨¡æ€è¾“å…¥ |
| languageInput | LanguageInput | è¯­è¨€æ¨¡æ€è¾“å…¥ |
| proprioceptionInput | ProprioceptionInput | æœ¬ä½“æ„ŸçŸ¥è¾“å…¥ |
| fusedFeatures | NdArray | èåˆåçš„å¤šæ¨¡æ€ç‰¹å¾ |
| attentionWeights | Map<String, NdArray> | å„æ¨¡æ€æ³¨æ„åŠ›æƒé‡ |
| timestamp | long | çŠ¶æ€æ—¶é—´æˆ³ |

#### 3.1.3 åŠ¨ä½œç©ºé—´å®šä¹‰

**VLAAction - VLAåŠ¨ä½œè¡¨ç¤º**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | å–å€¼èŒƒå›´ |
|-----|------|------|---------|
| continuousAction | NdArray | è¿ç»­åŠ¨ä½œå‘é‡ | å…·ä½“ä»»åŠ¡å®šä¹‰ |
| discreteAction | int | ç¦»æ•£åŠ¨ä½œç´¢å¼• | [0, num_actions) |
| actionType | ActionType | åŠ¨ä½œç±»å‹æšä¸¾ | MOVE/GRASP/RELEASE |
| confidence | double | åŠ¨ä½œç½®ä¿¡åº¦ | [0.0, 1.0] |
| languageFeedback | String | è‡ªç„¶è¯­è¨€åé¦ˆ | - |

**ActionType - åŠ¨ä½œç±»å‹æšä¸¾**

```
MOVE_END_EFFECTOR    // ç§»åŠ¨æœ«ç«¯æ‰§è¡Œå™¨
ROTATE_JOINTS        // æ—‹è½¬å…³èŠ‚
GRASP_OBJECT         // æŠ“å–ç‰©ä½“
RELEASE_OBJECT       // é‡Šæ”¾ç‰©ä½“
NAVIGATE_TO_TARGET   // å¯¼èˆªåˆ°ç›®æ ‡ç‚¹
WAIT                 // ç­‰å¾…
SPEAK                // è¯­è¨€è¾“å‡º
```

### 3.2 è§†è§‰-è¯­è¨€-åŠ¨ä½œç¼–ç å™¨

#### 3.2.1 è§†è§‰ç¼–ç å™¨æ¶æ„

**VisionEncoder - åŸºäºå·ç§¯ç¥ç»ç½‘ç»œä¸è§†è§‰Transformer**

**æ¶æ„æµç¨‹**ï¼š

```
è¾“å…¥å›¾åƒ [H, W, 3]
    â†“
å·ç§¯ç‰¹å¾æå–ï¼ˆResNet-styleï¼‰
    â”œâ”€ Conv2D (3â†’64, kernel=7, stride=2)
    â”œâ”€ MaxPool (kernel=3, stride=2)
    â”œâ”€ ResBlock Ã— 4 (64â†’128)
    â”œâ”€ ResBlock Ã— 4 (128â†’256)
    â””â”€ ResBlock Ã— 4 (256â†’512)
    â†“
ç©ºé—´ç‰¹å¾å›¾ [H/32, W/32, 512]
    â†“
ä½ç½®ç¼–ç æ³¨å…¥
    â†“
Flatten + LinearæŠ•å½±
    â†“
è§†è§‰Tokenåºåˆ— [196, 768]
```

**æ ¸å¿ƒç‰¹å¾**ï¼š
- åˆ†å±‚ç‰¹å¾æå–ï¼Œä¿ç•™ç©ºé—´ç»“æ„ä¿¡æ¯
- 2Dä½ç½®ç¼–ç ï¼Œå»ºæ¨¡ç©ºé—´å…³ç³»
- è¾“å‡ºç»´åº¦ä¸è¯­è¨€ç¼–ç å™¨å¯¹é½ï¼Œä¾¿äºè·¨æ¨¡æ€èåˆ

#### 3.2.2 è¯­è¨€ç¼–ç å™¨æ¶æ„

**LanguageEncoder - åŸºäºGPT Transformer**

**æ¶æ„æµç¨‹**ï¼š

```
è‡ªç„¶è¯­è¨€æŒ‡ä»¤
    â†“
Tokenizationï¼ˆBPE/WordPieceï¼‰
    â†“
Token Embedding [seq_len, 768]
    â†“
ä½ç½®ç¼–ç ï¼ˆæ­£å¼¦/å­¦ä¹ å¼ï¼‰
    â†“
Transformerç¼–ç å™¨ Ã— 6å±‚
    â”œâ”€ Multi-Head Self-Attention
    â”œâ”€ Layer Normalization
    â”œâ”€ Feed-Forward Network
    â””â”€ Residual Connection
    â†“
è¯­è¨€Tokenåºåˆ— [seq_len, 768]
    â†“
æŒ‡ä»¤åµŒå…¥å‘é‡ï¼ˆå–[CLS] tokenæˆ–mean poolingï¼‰
```

**å¤ç”¨ç­–ç•¥**ï¼š
- ç›´æ¥å¤ç”¨ `tinyai-model-gpt` ä¸­çš„ GPT Transformer Block
- å¤ç”¨ Tokenizer ä¸ Embedding å±‚
- å¯é€‰é¢„è®­ç»ƒæƒé‡åŠ è½½æœºåˆ¶

#### 3.2.3 æœ¬ä½“æ„ŸçŸ¥ç¼–ç å™¨

**ProprioceptionEncoder - MLPç¼–ç å™¨**

**æ¶æ„æµç¨‹**ï¼š

```
å…³èŠ‚çŠ¶æ€å‘é‡ [n_joints * 2]
    â†“
Linear Layer (input_dim â†’ 256)
    â†“
Layer Norm + ReLU
    â†“
Linear Layer (256 â†’ 512)
    â†“
Layer Norm + ReLU
    â†“
Linear Layer (512 â†’ 768)
    â†“
æœ¬ä½“æ„ŸçŸ¥åµŒå…¥ [1, 768]
```

### 3.3 è·¨æ¨¡æ€èåˆå±‚

#### 3.3.1 è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶

**CrossModalAttention - ä¸‰æ¨¡æ€èåˆ**

**æ³¨æ„åŠ›è®¡ç®—æµç¨‹**ï¼š

```
Query: è¯­è¨€ç‰¹å¾ [seq_len_lang, 768]
Key:   [è§†è§‰ç‰¹å¾; æœ¬ä½“ç‰¹å¾] [196+1, 768]
Value: [è§†è§‰ç‰¹å¾; æœ¬ä½“ç‰¹å¾] [196+1, 768]

Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

è¾“å‡º: è¯­è¨€å¼•å¯¼çš„å¤šæ¨¡æ€ç‰¹å¾ [seq_len_lang, 768]
```

**æ³¨æ„åŠ›ç±»å‹**ï¼š

| æ³¨æ„åŠ›ç±»å‹ | Queryæ¥æº | Key/Valueæ¥æº | ä½œç”¨ |
|-----------|---------|--------------|------|
| Vision-to-Language | è¯­è¨€Token | è§†è§‰Token | è¯­è¨€ç†è§£è§†è§‰åœºæ™¯ |
| Language-to-Vision | è§†è§‰Token | è¯­è¨€Token | è§†è§‰å…³æ³¨è¯­è¨€æŒ‡ä»¤ |
| Proprioception-to-All | æ‰€æœ‰æ¨¡æ€ | æœ¬ä½“æ„ŸçŸ¥ | å½“å‰çŠ¶æ€å…¨å±€æ„ŸçŸ¥ |

#### 3.3.2 ç»Ÿä¸€Transformerä¸»å¹²

**VLATransformerCore - å¤šå±‚èåˆTransformer**

**ç½‘ç»œç»“æ„**ï¼š

```
è¾“å…¥: æ‹¼æ¥çš„å¤šæ¨¡æ€Tokenåºåˆ—
[è¯­è¨€Token; è§†è§‰Token; æœ¬ä½“Token]
    â†“
Transformer Layer Ã— 12
    â”œâ”€ Multi-Head Self-Attention (å…¨å±€æ³¨æ„åŠ›)
    â”œâ”€ Cross-Modal Attention (è·¨æ¨¡æ€æ³¨æ„åŠ›)
    â”œâ”€ Feed-Forward Network (2048â†’768)
    â””â”€ Layer Norm + Residual
    â†“
èåˆç‰¹å¾è¡¨ç¤º [total_seq_len, 768]
```

**å…³é”®è®¾è®¡**ï¼š
- å…¨å±€è‡ªæ³¨æ„åŠ›ï¼šæ‰€æœ‰æ¨¡æ€Tokenå¯ä»¥äº’ç›¸å…³æ³¨
- ä½ç½®æ— å…³æ€§ï¼šé€šè¿‡å­¦ä¹ çš„æ¨¡æ€åµŒå…¥åŒºåˆ†ä¸åŒæ¥æº
- æ·±å±‚èåˆï¼šå¤šå±‚äº¤äº’ç¡®ä¿å……åˆ†çš„ä¿¡æ¯äº¤æ¢

### 3.4 åŠ¨ä½œè§£ç å™¨

#### 3.4.1 åŠ¨ä½œå¤´è®¾è®¡

**ActionDecoder - å¤šå¤´è¾“å‡ºæ¶æ„**

```
èåˆç‰¹å¾ [total_seq_len, 768]
    â†“
ç‰¹å¾èšåˆï¼ˆå–æœ€åä¸€ä¸ªTokenæˆ–Attention Poolingï¼‰
    â†“
    â”œâ”€ è¿ç»­åŠ¨ä½œå¤´
    â”‚   â””â”€ MLP (768â†’512â†’256â†’action_dim)
    â”‚       â””â”€ Tanhæ¿€æ´»ï¼ˆå½’ä¸€åŒ–åˆ°[-1, 1]ï¼‰
    â”‚
    â”œâ”€ ç¦»æ•£åŠ¨ä½œå¤´
    â”‚   â””â”€ Linear (768â†’num_discrete_actions)
    â”‚       â””â”€ Softmax
    â”‚
    â””â”€ è¯­è¨€åé¦ˆå¤´
        â””â”€ Transformer Decoder (ç”Ÿæˆå¼)
            â””â”€ æ–‡æœ¬åºåˆ—è¾“å‡º
```

**è¾“å‡ºç»´åº¦ç¤ºä¾‹**ï¼ˆæœºå™¨äººæ“ä½œä»»åŠ¡ï¼‰ï¼š

| è¾“å‡ºç±»å‹ | ç»´åº¦ | è¯´æ˜ |
|---------|-----|------|
| æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®å¢é‡ | 3 | (Î”x, Î”y, Î”z) |
| æœ«ç«¯æ‰§è¡Œå™¨æ—‹è½¬å¢é‡ | 3 | (Î”roll, Î”pitch, Î”yaw) |
| å¤¹çˆªå¼€åˆåº¦ | 1 | [0, 1] |
| ç¦»æ•£åŠ¨ä½œåˆ†ç±» | 8 | 8ç§åŸºç¡€åŠ¨ä½œç±»å‹ |

#### 3.4.2 åŠ¨ä½œåºåˆ—ç”Ÿæˆ

**æ”¯æŒè‡ªå›å½’åŠ¨ä½œç”Ÿæˆ**ï¼š

```
åˆå§‹çŠ¶æ€ s_0
    â†“
ç¼–ç å™¨è¾“å‡ºèåˆç‰¹å¾ h
    â†“
for t in range(T):
    a_t = Decoder(h, a_{t-1}, ...)
    æ‰§è¡ŒåŠ¨ä½œ a_t
    è§‚å¯Ÿæ–°çŠ¶æ€ s_{t+1}
    æ›´æ–°èåˆç‰¹å¾ hï¼ˆå¯é€‰ï¼‰
```

### 3.5 ç¯å¢ƒä»¿çœŸå±‚

#### 3.5.1 æœºå™¨äººç¯å¢ƒæ¥å£

**RobotEnvironment - ç»Ÿä¸€ç¯å¢ƒæ¥å£**

**æ ¸å¿ƒæ–¹æ³•**ï¼š

```
reset() â†’ VLAState
    é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€ï¼Œè¿”å›åˆå§‹è§‚æµ‹

step(VLAAction) â†’ (VLAState, reward, done, info)
    æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›æ–°çŠ¶æ€ã€å¥–åŠ±ã€ç»ˆæ­¢æ ‡å¿—å’Œé¢å¤–ä¿¡æ¯

render() â†’ Image
    æ¸²æŸ“å½“å‰ç¯å¢ƒå›¾åƒï¼ˆç”¨äºå¯è§†åŒ–ï¼‰

getActionSpace() â†’ ActionSpaceSpec
    è·å–åŠ¨ä½œç©ºé—´å®šä¹‰

getObservationSpace() â†’ ObservationSpaceSpec
    è·å–è§‚æµ‹ç©ºé—´å®šä¹‰

close()
    æ¸…ç†ç¯å¢ƒèµ„æº
```

#### 3.5.2 ä»»åŠ¡åœºæ™¯å®šä¹‰

**å†…ç½®ä»»åŠ¡åœºæ™¯**ï¼š

| ä»»åŠ¡ç±»å‹ | éš¾åº¦ | æè¿° | æˆåŠŸæ ‡å‡† |
|---------|------|------|---------|
| PickAndPlace | â­â­ | æ‹¾å–ç‰©ä½“å¹¶æ”¾ç½®åˆ°ç›®æ ‡ä½ç½® | ç‰©ä½“ä½äºç›®æ ‡åŒºåŸŸ |
| StackBlocks | â­â­â­ | å †å å¤šä¸ªæ–¹å— | æ–¹å—æŒ‰é¡ºåºå †å  |
| OpenDrawer | â­â­â­ | æ‰“å¼€æŠ½å±‰ | æŠ½å±‰æ‰“å¼€è§’åº¦>60Â° |
| PourWater | â­â­â­â­ | å€’æ°´ä»»åŠ¡ | ç›®æ ‡å®¹å™¨æ°´é‡è¾¾æ ‡ |
| AssembleParts | â­â­â­â­â­ | ç»„è£…é›¶ä»¶ | é›¶ä»¶æ­£ç¡®ç»„åˆ |

**åœºæ™¯é…ç½®ç¤ºä¾‹**ï¼š

```
PickAndPlaceä»»åŠ¡:
    ç‰©ä½“ç±»å‹: ç«‹æ–¹ä½“/çƒä½“/åœ†æŸ±ä½“
    åˆå§‹ä½ç½®: éšæœºèŒƒå›´ [0.3, 0.6] Ã— [âˆ’0.3, 0.3] Ã— [0.0, 0.2]
    ç›®æ ‡ä½ç½®: å›ºå®šåŒºåŸŸ [0.5, 0.5] Ã— [0.0, 0.0] Ã— [0.0, 0.0]
    è¯­è¨€æŒ‡ä»¤æ ·ä¾‹:
        - "Pick up the red cube and place it in the box"
        - "Grab the blue ball and move it to the left"
        - "æ‹¿èµ·çº¢è‰²æ–¹å—æ”¾åˆ°ç¯®å­é‡Œ"
```

### 3.6 å­¦ä¹ å¼•æ“

#### 3.6.1 å­¦ä¹ ç­–ç•¥

**VLALearningEngine - ç«¯åˆ°ç«¯å­¦ä¹ **

**æ”¯æŒçš„å­¦ä¹ èŒƒå¼**ï¼š

| å­¦ä¹ æ–¹å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|
| è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰ | ç›‘ç£å­¦ä¹ ï¼Œæ¨¡ä»¿ä¸“å®¶æ¼”ç¤º | æœ‰å¤§é‡æ ‡æ³¨æ¼”ç¤ºæ•°æ® |
| å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ | é€šè¿‡å¥–åŠ±ä¿¡å·å­¦ä¹ ç­–ç•¥ | å¯å®šä¹‰æ˜ç¡®å¥–åŠ±å‡½æ•° |
| é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰ | ä»æ¼”ç¤ºä¸­å­¦ä¹ å¥–åŠ±å‡½æ•° | å¥–åŠ±éš¾ä»¥äººå·¥è®¾è®¡ |
| æ··åˆå­¦ä¹  | BCé¢„è®­ç»ƒ + RLå¾®è°ƒ | å®é™…åº”ç”¨æ¨èæ–¹å¼ |

#### 3.6.2 è®­ç»ƒæµç¨‹

**è¡Œä¸ºå…‹éš†è®­ç»ƒæµç¨‹**ï¼š

```
1. æ•°æ®æ”¶é›†
   - æ”¶é›†ä¸“å®¶æ¼”ç¤ºè½¨è¿¹ D = {(s_i, a_i, instruction_i)}
   - æ•°æ®å¢å¼ºï¼ˆå›¾åƒå˜æ¢ã€æŒ‡ä»¤æ”¹å†™ï¼‰

2. æ¨¡å‹è®­ç»ƒ
   for epoch in range(num_epochs):
       for batch in DataLoader(D):
           vision_input = batch.images
           lang_input = batch.instructions
           target_actions = batch.actions
           
           # å‰å‘ä¼ æ’­
           predicted_actions = vla_model(vision_input, lang_input)
           
           # è®¡ç®—æŸå¤±
           loss = MSELoss(predicted_actions, target_actions)
           
           # åå‘ä¼ æ’­
           loss.backward()
           optimizer.step()

3. è¯„ä¼°ä¸éƒ¨ç½²
   - åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æˆåŠŸç‡
   - åœ¨çœŸå®ç¯å¢ƒä¸­æµ‹è¯•æ³›åŒ–èƒ½åŠ›
```

**å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹**ï¼š

```
1. ç¯å¢ƒäº¤äº’
   for episode in range(num_episodes):
       state = env.reset()
       trajectory = []
       
       for step in range(max_steps):
           action = vla_model.predict(state)
           next_state, reward, done = env.step(action)
           trajectory.append((state, action, reward))
           
           if done:
               break
           state = next_state

2. ç­–ç•¥æ›´æ–°ï¼ˆPPOç®—æ³•ï¼‰
   - è®¡ç®—ä¼˜åŠ¿å‡½æ•° A(s, a)
   - è®¡ç®—ç­–ç•¥æŸå¤± L_policy
   - è®¡ç®—ä»·å€¼æŸå¤± L_value
   - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–

3. è¿­ä»£ä¼˜åŒ–
   - å®šæœŸè¯„ä¼°ç­–ç•¥æ€§èƒ½
   - è°ƒæ•´è¶…å‚æ•°
   - ä¿å­˜æœ€ä½³æ¨¡å‹
```

## 4. å…³é”®ç®—æ³•å®ç°

### 4.1 è§†è§‰ç‰¹å¾æå–ç®—æ³•

**æ®‹å·®å·ç§¯å—ï¼ˆResBlockï¼‰**

```
è¾“å…¥ç‰¹å¾å›¾ F_in [B, C_in, H, W]
    â†“
åˆ†æ”¯1ï¼ˆæ’ç­‰æ˜ å°„ï¼‰:
    F_identity = F_in
    
åˆ†æ”¯2ï¼ˆå·ç§¯å˜æ¢ï¼‰:
    F_conv = Conv2D(C_in â†’ C_out, 3Ã—3)(F_in)
    F_conv = BatchNorm(F_conv)
    F_conv = ReLU(F_conv)
    F_conv = Conv2D(C_out â†’ C_out, 3Ã—3)(F_conv)
    F_conv = BatchNorm(F_conv)
    
åˆå¹¶:
    F_out = ReLU(F_identity + F_conv)
```

**ç©ºé—´ä½ç½®ç¼–ç **

```
å¯¹äºç‰¹å¾å›¾ä¸Šçš„æ¯ä¸ªä½ç½® (i, j):

PE(i, j, 2k)   = sin(i / 10000^(2k/d))
PE(i, j, 2k+1) = cos(i / 10000^(2k/d))
PE(i, j, d/2+2k)   = sin(j / 10000^(2k/d))
PE(i, j, d/2+2k+1) = cos(j / 10000^(2k/d))

æœ€ç»ˆä½ç½®ç¼–ç ç»´åº¦: [H, W, d]
```

### 4.2 è·¨æ¨¡æ€æ³¨æ„åŠ›è®¡ç®—

**ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**

```
è¾“å…¥:
    Q: QueryçŸ©é˜µ [batch, n_heads, seq_len_q, d_k]
    K: KeyçŸ©é˜µ   [batch, n_heads, seq_len_k, d_k]
    V: ValueçŸ©é˜µ [batch, n_heads, seq_len_v, d_v]

è®¡ç®—:
    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = (Q @ K^T) / sqrt(d_k)
    # scores: [batch, n_heads, seq_len_q, seq_len_k]
    
    # 2. åº”ç”¨æ©ç ï¼ˆå¯é€‰ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # 3. Softmaxå½’ä¸€åŒ–
    attn_weights = softmax(scores, dim=-1)
    
    # 4. åŠ æƒæ±‚å’Œ
    output = attn_weights @ V
    # output: [batch, n_heads, seq_len_q, d_v]
    
è¿”å›: output, attn_weights
```

### 4.3 åŠ¨ä½œå½’ä¸€åŒ–ä¸åå½’ä¸€åŒ–

**è¿ç»­åŠ¨ä½œå½’ä¸€åŒ–**

```
ç»™å®šåŠ¨ä½œç©ºé—´èŒƒå›´:
    action_low  = [-1.0, -1.0, -1.0, 0.0]  # æœ€å°å€¼
    action_high = [1.0,  1.0,  1.0,  1.0]  # æœ€å¤§å€¼

åŸå§‹åŠ¨ä½œ a_rawï¼ˆæ¨¡å‹è¾“å‡ºï¼ŒTanhåèŒƒå›´[-1, 1]ï¼‰

å½’ä¸€åŒ–åˆ°çœŸå®åŠ¨ä½œç©ºé—´:
    a_real = action_low + (a_raw + 1) / 2 * (action_high - action_low)

åå½’ä¸€åŒ–ï¼ˆç”¨äºç›‘ç£å­¦ä¹ ï¼‰:
    a_normalized = 2 * (a_real - action_low) / (action_high - action_low) - 1
```

### 4.4 å¥–åŠ±å‡½æ•°è®¾è®¡

**ç»„åˆå¥–åŠ±è®¾è®¡ï¼ˆPickAndPlaceä»»åŠ¡ï¼‰**

```
æ€»å¥–åŠ± R_total = Î£ w_i Â· R_i

åˆ†é¡¹å¥–åŠ±:

1. æ¥è¿‘ç›®æ ‡å¥–åŠ±
   R_reach = -distance(gripper, object)
   
2. æŠ“å–æˆåŠŸå¥–åŠ±
   R_grasp = +10.0  å¦‚æœæŠ“å–æˆåŠŸ
             0.0    å¦åˆ™
   
3. æ”¾ç½®æˆåŠŸå¥–åŠ±
   R_place = +50.0  å¦‚æœç‰©ä½“åœ¨ç›®æ ‡åŒºåŸŸ
             0.0    å¦åˆ™
   
4. åŠ¨ä½œå¹³æ»‘å¥–åŠ±
   R_smooth = -|a_t - a_{t-1}|
   
5. ç¢°æ’æƒ©ç½š
   R_collision = -20.0  å¦‚æœå‘ç”Ÿç¢°æ’
                  0.0   å¦åˆ™

æƒé‡é…ç½®:
    w_reach = 1.0
    w_grasp = 10.0
    w_place = 50.0
    w_smooth = 0.1
    w_collision = 20.0
```

## 5. æ¨¡å—ä¾èµ–ä¸é›†æˆ

### 5.1 å†…éƒ¨æ¨¡å—ä¾èµ–

| ä¾èµ–æ¨¡å— | ä½¿ç”¨ç»„ä»¶ | ç”¨é€” |
|---------|---------|------|
| tinyai-deeplearning-ndarr | NdArray | å¤šç»´æ•°ç»„è¿ç®— |
| tinyai-deeplearning-func | Variable, AutoGrad | è‡ªåŠ¨å¾®åˆ†ä¸æ¢¯åº¦è®¡ç®— |
| tinyai-deeplearning-nnet | Linear, Conv2D, LayerNorm | ç¥ç»ç½‘ç»œå±‚ |
| tinyai-deeplearning-ml | Optimizer, LossFunction | è®­ç»ƒä¼˜åŒ– |
| tinyai-deeplearning-rl | PolicyGradient, ValueNetwork | å¼ºåŒ–å­¦ä¹ ç®—æ³• |
| tinyai-model-gpt | GPT Transformer, Attention | Transformeréª¨å¹²ç½‘ç»œ |
| tinyai-agent-embodied | VehicleDynamics, Environmentæ¥å£ | ç¯å¢ƒä»¿çœŸå‚è€ƒ |

### 5.2 Mavenä¾èµ–é…ç½®

```xml
<dependencies>
    <!-- TinyAI æ·±åº¦å­¦ä¹ æ ¸å¿ƒæ¨¡å— -->
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-ndarr</artifactId>
    </dependency>
    
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-func</artifactId>
    </dependency>
    
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-nnet</artifactId>
    </dependency>
    
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-ml</artifactId>
    </dependency>
    
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-deeplearning-rl</artifactId>
    </dependency>
    
    <!-- TinyAI æ¨¡å‹å±‚ -->
    <dependency>
        <groupId>io.leavesfly.tinyai</groupId>
        <artifactId>tinyai-model-gpt</artifactId>
    </dependency>
    
    <!-- æµ‹è¯•ä¾èµ– -->
    <dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <scope>test</scope>
    </dependency>
    
    <dependency>
        <groupId>org.junit.jupiter</groupId>
        <artifactId>junit-jupiter</artifactId>
        <scope>test</scope>
    </dependency>
</dependencies>
```

### 5.3 æ¨¡å—ç›®å½•ç»“æ„

```
tinyai-agent-embodied-vla/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ java/io/leavesfly/tinyai/agent/vla/
â”‚   â”‚       â”œâ”€â”€ VLAAgent.java                    # VLAæ™ºèƒ½ä½“æ ¸å¿ƒ
â”‚   â”‚       â”œâ”€â”€ VLADemo.java                     # æ¼”ç¤ºç¨‹åº
â”‚   â”‚       â”œâ”€â”€ encoder/                         # ç¼–ç å™¨æ¨¡å—
â”‚   â”‚       â”‚   â”œâ”€â”€ VisionEncoder.java           # è§†è§‰ç¼–ç å™¨
â”‚   â”‚       â”‚   â”œâ”€â”€ LanguageEncoder.java         # è¯­è¨€ç¼–ç å™¨
â”‚   â”‚       â”‚   â””â”€â”€ ProprioceptionEncoder.java   # æœ¬ä½“æ„ŸçŸ¥ç¼–ç å™¨
â”‚   â”‚       â”œâ”€â”€ fusion/                          # èåˆæ¨¡å—
â”‚   â”‚       â”‚   â”œâ”€â”€ CrossModalAttention.java     # è·¨æ¨¡æ€æ³¨æ„åŠ›
â”‚   â”‚       â”‚   â””â”€â”€ VLATransformerCore.java      # Transformeræ ¸å¿ƒ
â”‚   â”‚       â”œâ”€â”€ decoder/                         # è§£ç å™¨æ¨¡å—
â”‚   â”‚       â”‚   â”œâ”€â”€ ActionDecoder.java           # åŠ¨ä½œè§£ç å™¨
â”‚   â”‚       â”‚   â””â”€â”€ LanguageFeedbackGenerator.java # è¯­è¨€åé¦ˆç”Ÿæˆ
â”‚   â”‚       â”œâ”€â”€ env/                             # ç¯å¢ƒæ¨¡å—
â”‚   â”‚       â”‚   â”œâ”€â”€ RobotEnvironment.java        # æœºå™¨äººç¯å¢ƒæ¥å£
â”‚   â”‚       â”‚   â”œâ”€â”€ SimpleRobotEnv.java          # ç®€å•æœºå™¨äººç¯å¢ƒ
â”‚   â”‚       â”‚   â””â”€â”€ TaskScenario.java            # ä»»åŠ¡åœºæ™¯å®šä¹‰
â”‚   â”‚       â”œâ”€â”€ learning/                        # å­¦ä¹ æ¨¡å—
â”‚   â”‚       â”‚   â”œâ”€â”€ VLALearningEngine.java       # å­¦ä¹ å¼•æ“
â”‚   â”‚       â”‚   â”œâ”€â”€ BehaviorCloningLearner.java  # è¡Œä¸ºå…‹éš†
â”‚   â”‚       â”‚   â””â”€â”€ RLLearner.java               # å¼ºåŒ–å­¦ä¹ 
â”‚   â”‚       â”œâ”€â”€ model/                           # æ•°æ®æ¨¡å‹
â”‚   â”‚       â”‚   â”œâ”€â”€ VLAState.java                # VLAçŠ¶æ€
â”‚   â”‚       â”‚   â”œâ”€â”€ VLAAction.java               # VLAåŠ¨ä½œ
â”‚   â”‚       â”‚   â”œâ”€â”€ VisionInput.java             # è§†è§‰è¾“å…¥
â”‚   â”‚       â”‚   â”œâ”€â”€ LanguageInput.java           # è¯­è¨€è¾“å…¥
â”‚   â”‚       â”‚   â”œâ”€â”€ ProprioceptionInput.java     # æœ¬ä½“æ„ŸçŸ¥è¾“å…¥
â”‚   â”‚       â”‚   â””â”€â”€ TaskConfig.java              # ä»»åŠ¡é…ç½®
â”‚   â”‚       â””â”€â”€ utils/                           # å·¥å…·ç±»
â”‚   â”‚           â”œâ”€â”€ ImageProcessor.java          # å›¾åƒå¤„ç†
â”‚   â”‚           â”œâ”€â”€ Tokenizer.java               # æ–‡æœ¬åˆ†è¯
â”‚   â”‚           â””â”€â”€ ActionNormalizer.java        # åŠ¨ä½œå½’ä¸€åŒ–
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ java/io/leavesfly/tinyai/agent/vla/
â”‚           â”œâ”€â”€ VLAAgentTest.java
â”‚           â”œâ”€â”€ encoder/
â”‚           â”œâ”€â”€ fusion/
â”‚           â””â”€â”€ decoder/
â”œâ”€â”€ doc/
â”‚   â”œâ”€â”€ æŠ€æœ¯æ¶æ„æ–‡æ¡£.md
â”‚   â””â”€â”€ ä½¿ç”¨æŒ‡å—.md
â”œâ”€â”€ README.md
â””â”€â”€ pom.xml
```

## 6. æ•°æ®æµä¸äº¤äº’æµç¨‹

### 6.1 å®Œæ•´æ¨ç†æµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Agent as VLAAgent
    participant Encoder as ç¼–ç å™¨å±‚
    participant Fusion as èåˆå±‚
    participant Decoder as è§£ç å™¨å±‚
    participant Env as æœºå™¨äººç¯å¢ƒ
    
    User->>Agent: è¾“å…¥æŒ‡ä»¤ + å›¾åƒ
    Agent->>Encoder: å¤šæ¨¡æ€ç¼–ç 
    
    Encoder->>Encoder: è§†è§‰ç¼–ç 
    Encoder->>Encoder: è¯­è¨€ç¼–ç 
    Encoder->>Encoder: æœ¬ä½“æ„ŸçŸ¥ç¼–ç 
    
    Encoder->>Fusion: è¿”å›å„æ¨¡æ€ç‰¹å¾
    Fusion->>Fusion: è·¨æ¨¡æ€æ³¨æ„åŠ›è®¡ç®—
    Fusion->>Fusion: Transformerèåˆ
    
    Fusion->>Decoder: è¿”å›èåˆç‰¹å¾
    Decoder->>Decoder: åŠ¨ä½œè§£ç 
    Decoder->>Decoder: è¯­è¨€åé¦ˆç”Ÿæˆ
    
    Decoder->>Agent: è¿”å›åŠ¨ä½œ+åé¦ˆ
    Agent->>Env: æ‰§è¡ŒåŠ¨ä½œ
    Env->>Agent: è¿”å›æ–°çŠ¶æ€+å¥–åŠ±
    Agent->>User: åé¦ˆæ‰§è¡Œç»“æœ
```

### 6.2 è®­ç»ƒæ•°æ®æµ

```
ä¸“å®¶æ¼”ç¤ºæ•°æ®
    â”œâ”€ å›¾åƒåºåˆ—: [T, H, W, 3]
    â”œâ”€ è¯­è¨€æŒ‡ä»¤: String
    â”œâ”€ å…³èŠ‚çŠ¶æ€åºåˆ—: [T, n_joints]
    â””â”€ åŠ¨ä½œåºåˆ—: [T, action_dim]
    
    â†“ æ•°æ®é¢„å¤„ç†
    
è®­ç»ƒBatch
    â”œâ”€ vision_batch: [B, T, H, W, 3]
    â”œâ”€ language_batch: [B, seq_len]
    â”œâ”€ proprio_batch: [B, T, n_joints]
    â””â”€ action_batch: [B, T, action_dim]
    
    â†“ å‰å‘ä¼ æ’­
    
é¢„æµ‹åŠ¨ä½œ: [B, T, action_dim]
    
    â†“ æŸå¤±è®¡ç®—
    
Loss = MSE(predicted_actions, target_actions)
     + Î»1 * L2_regularization
     + Î»2 * action_smoothness_loss
     
    â†“ åå‘ä¼ æ’­
    
æ›´æ–°æ¨¡å‹å‚æ•°
```

### 6.3 ç¯å¢ƒäº¤äº’å¾ªç¯

```
åˆå§‹åŒ–:
    env = SimpleRobotEnv(task_config)
    agent = VLAAgent(model_config)
    instruction = "Pick up the red cube"

å•ä¸ªEpisode:
    state = env.reset()
    done = False
    total_reward = 0
    
    while not done:
        # 1. è·å–è§‚æµ‹
        vision_input = state.vision
        proprio_input = state.proprioception
        
        # 2. æ™ºèƒ½ä½“å†³ç­–
        action, feedback = agent.predict(
            vision_input, 
            instruction, 
            proprio_input
        )
        
        # 3. æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, info = env.step(action)
        
        # 4. è®°å½•ä¸å­¦ä¹ 
        total_reward += reward
        agent.store_transition(state, action, reward, next_state)
        
        # 5. çŠ¶æ€æ›´æ–°
        state = next_state
        
    print(f"Episode finished: total_reward = {total_reward}")
```

## 7. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 7.1 è®¡ç®—æ•ˆç‡ä¼˜åŒ–

| ä¼˜åŒ–æ–¹å‘ | å…·ä½“æªæ–½ | é¢„æœŸæå‡ |
|---------|---------|---------|
| è§†è§‰ç¼–ç åŠ é€Ÿ | å›¾åƒç¼“å­˜ã€æ‰¹é‡å¤„ç† | 30% |
| æ³¨æ„åŠ›è®¡ç®—ä¼˜åŒ– | Flash Attentionã€ç¨€ç–æ³¨æ„åŠ› | 50% |
| æ¨¡å‹å‰ªæ | ç§»é™¤å†—ä½™å±‚ã€çŸ¥è¯†è’¸é¦ | 40% |
| æ‰¹å¤„ç†æ¨ç† | åŠ¨æ€Batchåˆå¹¶ | 2-3x |

### 7.2 å†…å­˜ä¼˜åŒ–

- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šåœ¨Transformerå±‚ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå‡å°‘50%æ˜¾å­˜
- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨FP16/BF16ï¼Œå‡å°‘å†…å­˜å ç”¨
- **åŠ¨æ€å›¾å‰ªæ**ï¼šåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„è®¡ç®—å›¾èŠ‚ç‚¹
- **ç‰¹å¾å¤ç”¨**ï¼šç¼“å­˜ä¸å˜çš„ç¼–ç ç‰¹å¾ï¼ˆå¦‚è¯­è¨€æŒ‡ä»¤ï¼‰

### 7.3 è®­ç»ƒåŠ é€Ÿ

- **è¯¾ç¨‹å­¦ä¹ **ï¼šä»ç®€å•ä»»åŠ¡é€æ­¥å¢åŠ éš¾åº¦
- **é¢„è®­ç»ƒè¿ç§»**ï¼šåˆ©ç”¨GPTé¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–
- **å¹¶è¡Œè®­ç»ƒ**ï¼šå¤šç¯å¢ƒå¹¶è¡Œé‡‡æ ·æ•°æ®
- **å¼‚æ­¥æ›´æ–°**ï¼šç¯å¢ƒäº¤äº’ä¸æ¨¡å‹æ›´æ–°å¼‚æ­¥è¿›è¡Œ

## 8. æ‰©å±•æ€§è®¾è®¡

### 8.1 æ–°æ¨¡æ€æ¥å…¥

```
æ·»åŠ è§¦è§‰ä¼ æ„Ÿå™¨:

1. å®šä¹‰è§¦è§‰è¾“å…¥æ¨¡å‹
   TactileInput:
       forceSensor: NdArray [n_sensors]
       pressureMap: NdArray [H, W]

2. å®ç°è§¦è§‰ç¼–ç å™¨
   TactileEncoder extends Encoder:
       encode(TactileInput) â†’ NdArray [seq_len, 768]

3. æ³¨å†Œåˆ°èåˆå±‚
   CrossModalFusion.registerModality("tactile", tactileEncoder)
```

### 8.2 æ–°ä»»åŠ¡åœºæ™¯

```
æ·»åŠ "å€’æ°´"ä»»åŠ¡:

1. å®šä¹‰ä»»åŠ¡é…ç½®
   PourWaterTask extends TaskScenario:
       container: GameObject
       targetContainer: GameObject
       waterVolume: double

2. å®ç°å¥–åŠ±å‡½æ•°
   reward = w1 * (-distance_to_target)
          + w2 * (water_poured_correctly)
          - w3 * (water_spilled)

3. åœºæ™¯æ³¨å†Œ
   TaskRegistry.register("pour_water", PourWaterTask.class)
```

### 8.3 æ–°å­¦ä¹ ç®—æ³•

```
æ·»åŠ ç¦»çº¿å¼ºåŒ–å­¦ä¹ :

1. å®ç°ç¦»çº¿RLå­¦ä¹ å™¨
   OfflineRLLearner extends VLALearner:
       learnFromDataset(Dataset) â†’ Policy

2. æ•°æ®é›†åŠ è½½
   dataset = loadDemonstrations("expert_demos.h5")

3. è®­ç»ƒæµç¨‹
   learner.train(dataset, num_epochs=100)
```

## 9. æµ‹è¯•ç­–ç•¥

### 9.1 å•å…ƒæµ‹è¯•

| æµ‹è¯•æ¨¡å— | æµ‹è¯•å†…å®¹ | è¦†ç›–ç›®æ ‡ |
|---------|---------|---------|
| VisionEncoder | ç‰¹å¾ç»´åº¦ã€æ•°å€¼èŒƒå›´ | 100% |
| LanguageEncoder | Tokenizationã€Embedding | 100% |
| CrossModalAttention | æ³¨æ„åŠ›æƒé‡å½’ä¸€åŒ– | 100% |
| ActionDecoder | åŠ¨ä½œèŒƒå›´ã€ç±»å‹æ­£ç¡®æ€§ | 100% |
| RobotEnvironment | çŠ¶æ€è½¬ç§»ã€å¥–åŠ±è®¡ç®— | 100% |

### 9.2 é›†æˆæµ‹è¯•

- **ç«¯åˆ°ç«¯æ¨ç†æµ‹è¯•**ï¼šéªŒè¯ä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´æµç¨‹
- **è®­ç»ƒæ”¶æ•›æµ‹è¯•**ï¼šéªŒè¯ç®€å•ä»»åŠ¡èƒ½å¤ŸæˆåŠŸå­¦ä¹ 
- **å¤šä»»åŠ¡æ³›åŒ–æµ‹è¯•**ï¼šéªŒè¯åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°
- **é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•**ï¼šéªŒè¯è¿ç»­è¿è¡Œä¸ä¼šå†…å­˜æ³„æ¼

### 9.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

| æŒ‡æ ‡ | åŸºå‡†å€¼ | è¯´æ˜ |
|-----|--------|------|
| æ¨ç†å»¶è¿Ÿ | < 50ms | å•æ¬¡åŠ¨ä½œé¢„æµ‹æ—¶é—´ |
| è®­ç»ƒé€Ÿåº¦ | > 100 steps/s | è®­ç»ƒååé‡ |
| æˆåŠŸç‡ï¼ˆç®€å•ä»»åŠ¡ï¼‰ | > 90% | PickAndPlaceæˆåŠŸç‡ |
| æˆåŠŸç‡ï¼ˆå¤æ‚ä»»åŠ¡ï¼‰ | > 60% | AssemblePartsæˆåŠŸç‡ |

## 10. åº”ç”¨åœºæ™¯

### 10.1 æœºå™¨äººæ“ä½œ

- **ç‰©ä½“æŠ“å–ä¸æ”¾ç½®**ï¼šæ‹¾å–ä»»æ„ç‰©ä½“å¹¶æŒ‰æŒ‡ä»¤æ”¾ç½®
- **ç»„è£…ä»»åŠ¡**ï¼šç»„è£…ç®€å•é›¶ä»¶ï¼ˆä¹é«˜ã€å®¶å…·ï¼‰
- **å·¥å…·ä½¿ç”¨**ï¼šä½¿ç”¨å·¥å…·å®Œæˆä»»åŠ¡ï¼ˆæ‰³æ‰‹ã€èºä¸åˆ€ï¼‰

### 10.2 äººæœºåä½œ

- **è¾…åŠ©è£…é…**ï¼šååŠ©äººç±»å®Œæˆè£…é…å·¥ä½œ
- **ç‰©æ–™æ¬è¿**ï¼šæ ¹æ®æŒ‡ä»¤æ¬è¿ç‰©æ–™
- **è´¨é‡æ£€æŸ¥**ï¼šæ£€æŸ¥äº§å“è´¨é‡å¹¶åé¦ˆ

### 10.3 æœåŠ¡æœºå™¨äºº

- **å®¶åº­æœåŠ¡**ï¼šæ•´ç†æˆ¿é—´ã€æ”¶æ‹¾ç‰©å“
- **é¤é¥®æœåŠ¡**ï¼šç«¯é€é¤é¥®ã€æ¸…ç†æ¡Œé¢
- **åŒ»ç–—è¾…åŠ©**ï¼šè¾…åŠ©åŒ»æŠ¤äººå‘˜å®Œæˆç®€å•æ“ä½œ

## 11. æŠ€æœ¯äº®ç‚¹

### 11.1 åˆ›æ–°ç‚¹

- **ç»Ÿä¸€å¤šæ¨¡æ€å»ºæ¨¡**ï¼šè§†è§‰ã€è¯­è¨€ã€åŠ¨ä½œåœ¨ç»Ÿä¸€Transformeræ¡†æ¶ä¸‹å»ºæ¨¡
- **è¯­è¨€å¼•å¯¼æ³¨æ„åŠ›**ï¼šè‡ªç„¶è¯­è¨€æŒ‡ä»¤åŠ¨æ€è°ƒèŠ‚è§†è§‰æ³¨æ„åŠ›æƒé‡
- **ç«¯åˆ°ç«¯ä¼˜åŒ–**ï¼šä»æ„ŸçŸ¥åˆ°åŠ¨ä½œçš„å…¨é“¾è·¯å¯å¾®åˆ†å­¦ä¹ 
- **é›¶æ ·æœ¬æ³›åŒ–**ï¼šé€šè¿‡è¯­è¨€æŒ‡ä»¤ç»„åˆå®Œæˆæœªè§è¿‡çš„æ–°ä»»åŠ¡

### 11.2 å·¥ç¨‹ä¼˜åŠ¿

- **çº¯Javaå®ç°**ï¼šå®Œå…¨åŸºäºTinyAIç”Ÿæ€ï¼Œæ— å¤–éƒ¨ä¾èµ–
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶æ¾è€¦åˆï¼Œæ˜“äºæ›¿æ¢å’Œæ‰©å±•
- **é«˜åº¦å¯å¤ç”¨**ï¼šå……åˆ†å¤ç”¨GPTã€RLç­‰å·²æœ‰æ¨¡å—
- **æ–‡æ¡£å®Œå–„**ï¼šè¯¦ç»†çš„è®¾è®¡æ–‡æ¡£ä¸ä»£ç æ³¨é‡Š

### 11.3 æ•™è‚²ä»·å€¼

- **VLAæ¶æ„ç†è§£**ï¼šæ·±å…¥ç†è§£å‰æ²¿VLAæ¨¡å‹åŸç†
- **å¤šæ¨¡æ€å­¦ä¹ **ï¼šæŒæ¡å¤šæ¨¡æ€èåˆæŠ€æœ¯
- **å…·èº«æ™ºèƒ½å®è·µ**ï¼šä½“éªŒå®Œæ•´çš„å…·èº«æ™ºèƒ½å¼€å‘æµç¨‹
- **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šç†è§£ç«¯åˆ°ç«¯å­¦ä¹ çš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜
