# TinyAI VLA æœ€ä½³å®è·µæŒ‡å—

> æœ¬æŒ‡å—æ±‡æ€»äº†VLAå…·èº«æ™ºèƒ½ç³»ç»Ÿå¼€å‘å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³å®è·µå’Œç»éªŒå»ºè®®

## ğŸ“‹ ç›®å½•

- [1. ä»»åŠ¡è®¾è®¡æœ€ä½³å®è·µ](#1-ä»»åŠ¡è®¾è®¡æœ€ä½³å®è·µ)
- [2. æ¨¡å‹è®­ç»ƒæœ€ä½³å®è·µ](#2-æ¨¡å‹è®­ç»ƒæœ€ä½³å®è·µ)
- [3. è¶…å‚æ•°è°ƒä¼˜æœ€ä½³å®è·µ](#3-è¶…å‚æ•°è°ƒä¼˜æœ€ä½³å®è·µ)
- [4. æ•°æ®å¤„ç†æœ€ä½³å®è·µ](#4-æ•°æ®å¤„ç†æœ€ä½³å®è·µ)
- [5. æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ](#5-æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ)
- [6. æ¨¡å‹è¯„ä¼°æœ€ä½³å®è·µ](#6-æ¨¡å‹è¯„ä¼°æœ€ä½³å®è·µ)
- [7. ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ](#7-ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ)
- [8. å¸¸è§é™·é˜±é¿å…](#8-å¸¸è§é™·é˜±é¿å…)

---

## 1. ä»»åŠ¡è®¾è®¡æœ€ä½³å®è·µ

### 1.1 å¥–åŠ±å‡½æ•°è®¾è®¡

#### âœ… æ¨èåšæ³•

**DO: ä½¿ç”¨ç¨ å¯†å¥–åŠ±**
```java
// âœ“ å¥½çš„è®¾è®¡ï¼šæä¾›å¯†é›†çš„ä¸­é—´åé¦ˆ
double reward = 0.0;

// è·ç¦»ç›®æ ‡è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜
double distanceToTarget = calculateDistance(currentPos, targetPos);
reward += Math.max(0, 1.0 - distanceToTarget);

// æ­£ç¡®çš„åŠ¨ä½œæ–¹å‘é¢å¤–å¥–åŠ±
if (movingTowardsTarget) {
    reward += 0.1;
}

// æˆåŠŸå®Œæˆä»»åŠ¡å¤§å¥–åŠ±
if (taskCompleted) {
    reward += 100.0;
}
```

**DON'T: åªä½¿ç”¨ç¨€ç–å¥–åŠ±**
```java
// âœ— ä¸å¥½çš„è®¾è®¡ï¼šåªåœ¨æˆåŠŸæ—¶ç»™å¥–åŠ±
double reward = taskCompleted ? 100.0 : 0.0;
// é—®é¢˜ï¼šæ™ºèƒ½ä½“å¾ˆéš¾å­¦åˆ°æœ‰ç”¨çš„ç­–ç•¥
```

#### æœ€ä½³å®è·µæ€»ç»“

| åŸåˆ™ | è¯´æ˜ | ç¤ºä¾‹ |
|-----|------|------|
| **å¯†é›†åé¦ˆ** | æ¯æ­¥éƒ½æä¾›å¥–åŠ±ä¿¡å· | è·ç¦»å¥–åŠ±ã€æ–¹å‘å¥–åŠ± |
| **å¥–åŠ±å¡‘å½¢** | å¼•å¯¼æ™ºèƒ½ä½“æ‰¾åˆ°æ­£ç¡®è·¯å¾„ | åˆ†é˜¶æ®µå¥–åŠ±è®¾è®¡ |
| **é¿å…å¥–åŠ±é»‘å®¢** | ç¡®ä¿å¥–åŠ±ä¸ç›®æ ‡ä¸€è‡´ | æ£€æŸ¥æ„å¤–çš„é«˜å¥–åŠ±è¡Œä¸º |
| **å¥–åŠ±å½’ä¸€åŒ–** | ä¿æŒå¥–åŠ±åœ¨åˆç†èŒƒå›´ | [-1, 1] æˆ– [0, 100] |

---

### 1.2 ä»»åŠ¡åœºæ™¯é…ç½®

#### âœ… æ¨èåšæ³•

**DO: ä½¿ç”¨æ¸è¿›å¼éš¾åº¦**
```java
// âœ“ è¯¾ç¨‹å­¦ä¹ ï¼šä»ç®€å•åˆ°å¤æ‚
TaskConfig easyConfig = new TaskConfig();
easyConfig.setMaxSteps(50);  // æ›´å¤šæ—¶é—´
easyConfig.setObjectSize(0.1);  // æ›´å¤§çš„ç‰©ä½“
easyConfig.setTargetTolerance(0.05);  // æ›´å®½æ¾çš„å®¹å·®

TaskConfig hardConfig = new TaskConfig();
hardConfig.setMaxSteps(30);  // æ›´å°‘æ—¶é—´
hardConfig.setObjectSize(0.03);  // æ›´å°çš„ç‰©ä½“
hardConfig.setTargetTolerance(0.01);  // æ›´ä¸¥æ ¼çš„å®¹å·®
```

#### åœºæ™¯å¤šæ ·æ€§

```java
// âœ“ å¢åŠ åœºæ™¯éšæœºæ€§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
public VLAState reset() {
    // éšæœºåŒ–ç‰©ä½“ä½ç½®
    double[] objectPos = randomPosition(workspace);
    
    // éšæœºåŒ–ç›®æ ‡ä½ç½®
    double[] targetPos = randomPosition(workspace);
    
    // éšæœºåŒ–ç‰©ä½“å±æ€§
    ObjectProperties properties = new ObjectProperties();
    properties.setMass(randomMass(0.1, 1.0));
    properties.setFriction(randomFriction(0.3, 0.8));
    
    return new VLAState(...);
}
```

---

### 1.3 è¯­è¨€æŒ‡ä»¤è®¾è®¡

#### âœ… æ¨èåšæ³•

**DO: æä¾›æ¸…æ™°æ˜ç¡®çš„æŒ‡ä»¤**
```java
// âœ“ å¥½çš„æŒ‡ä»¤è®¾è®¡
String[] goodInstructions = {
    "Pick up the red cube and place it on the blue platform",
    "Grasp the object at position (0.5, 0.3) and move it to (0.2, 0.8)",
    "Stack the small block on top of the large block carefully"
};
```

**DON'T: ä½¿ç”¨æ¨¡ç³Šä¸æ¸…çš„æŒ‡ä»¤**
```java
// âœ— ä¸å¥½çš„æŒ‡ä»¤
String[] badInstructions = {
    "Do something",  // å¤ªæ¨¡ç³Š
    "Move it there",  // ç¼ºå°‘å…·ä½“ä¿¡æ¯
    "Just finish the task"  // æ²¡æœ‰æŒ‡å¯¼æ„ä¹‰
};
```

#### æŒ‡ä»¤æ¨¡æ¿åº“

```java
// å»ºç«‹æŒ‡ä»¤æ¨¡æ¿åº“ï¼Œå¢åŠ å¤šæ ·æ€§
Map<TaskType, List<String>> instructionTemplates = Map.of(
    PICK_AND_PLACE, List.of(
        "Pick up the {color} {object} and place it at {target}",
        "Grasp the {object} and move it to {target}",
        "Transfer the {color} {object} from {source} to {target}"
    ),
    STACK_BLOCKS, List.of(
        "Stack the {object1} on top of {object2}",
        "Build a tower with {num} blocks",
        "Place {object1} above {object2} carefully"
    )
);
```

---

## 2. æ¨¡å‹è®­ç»ƒæœ€ä½³å®è·µ

### 2.1 è®­ç»ƒæµç¨‹

#### âœ… æ ‡å‡†è®­ç»ƒæµç¨‹

```java
public void trainModel() {
    // 1. æ•°æ®æ”¶é›†ä¸é¢„å¤„ç†
    List<Demonstration> demos = collectDemonstrations(numDemos);
    
    // 2. æ•°æ®å¢å¼º
    demos = augmentData(demos);
    
    // 3. é¢„è®­ç»ƒï¼ˆå¦‚æœæœ‰é¢„è®­ç»ƒæ•°æ®ï¼‰
    if (hasPretrainedData) {
        agent.pretrainFromDemonstrations(demos);
    }
    
    // 4. åœ¨çº¿è®­ç»ƒ
    for (int episode = 0; episode < maxEpisodes; episode++) {
        // è®­ç»ƒä¸€ä¸ªå›åˆ
        double reward = learner.trainEpisode(agent, env);
        
        // å®šæœŸè¯„ä¼°
        if (episode % evalInterval == 0) {
            double evalReward = evaluate(agent, env, numEvalEpisodes);
            
            // ä¿å­˜æœ€ä½³æ¨¡å‹
            if (evalReward > bestReward) {
                bestReward = evalReward;
                agent.save("best_model.pth");
            }
        }
        
        // å­¦ä¹ ç‡è¡°å‡
        if (episode % lrDecayInterval == 0) {
            learner.decayLearningRate(0.9);
        }
    }
    
    // 5. æœ€ç»ˆè¯„ä¼°
    finalEvaluation(agent, env);
}
```

---

### 2.2 å­¦ä¹ ç‡è°ƒåº¦

#### âœ… æ¨èç­–ç•¥

**ç­–ç•¥1: é˜¶æ¢¯å¼è¡°å‡**
```java
public double getStepLR(int episode, double initialLR) {
    // æ¯50ä¸ªå›åˆå‡åŠ
    int decaySteps = episode / 50;
    return initialLR * Math.pow(0.5, decaySteps);
}
```

**ç­–ç•¥2: ä½™å¼¦é€€ç«**
```java
public double getCosineLR(int episode, int maxEpisodes, 
                          double initialLR, double minLR) {
    double cosine = Math.cos(Math.PI * episode / maxEpisodes);
    return minLR + (initialLR - minLR) * 0.5 * (1.0 + cosine);
}
```

**ç­–ç•¥3: Warm-up + çº¿æ€§è¡°å‡**
```java
public double getWarmupLR(int episode, double initialLR) {
    int warmupEpisodes = 10;
    
    if (episode < warmupEpisodes) {
        // Warm-upé˜¶æ®µ
        return initialLR * (episode + 1) / warmupEpisodes;
    } else {
        // çº¿æ€§è¡°å‡
        return initialLR * (1.0 - (episode - warmupEpisodes) / 
                           (maxEpisodes - warmupEpisodes));
    }
}
```

---

### 2.3 æ‰¹æ¬¡è®­ç»ƒ

#### âœ… æ¨èåšæ³•

```java
// âœ“ ä½¿ç”¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™
public void trainWithMiniBatch(List<StateActionPair> buffer) {
    int batchSize = 32;
    
    // æ‰“ä¹±æ•°æ®
    Collections.shuffle(buffer);
    
    // æ‰¹æ¬¡è®­ç»ƒ
    for (int i = 0; i < buffer.size(); i += batchSize) {
        List<StateActionPair> batch = buffer.subList(
            i, Math.min(i + batchSize, buffer.size())
        );
        
        // å‰å‘ä¼ æ’­
        List<VLAAction> predictions = agent.batchPredict(batch);
        
        // è®¡ç®—æŸå¤±
        double loss = computeLoss(predictions, batch);
        
        // åå‘ä¼ æ’­
        agent.backward(loss);
        
        // æ›´æ–°å‚æ•°
        optimizer.step();
    }
}
```

---

## 3. è¶…å‚æ•°è°ƒä¼˜æœ€ä½³å®è·µ

### 3.1 è¶…å‚æ•°æœç´¢ç©ºé—´

#### æ¨èæœç´¢èŒƒå›´

| è¶…å‚æ•° | æ¨èèŒƒå›´ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|---------|--------|------|
| **learning_rate** | [1e-5, 1e-2] | 1e-3 | æœ€é‡è¦çš„è¶…å‚æ•° |
| **batch_size** | [16, 128] | 32 | å—å†…å­˜é™åˆ¶ |
| **hidden_dim** | [256, 1024] | 768 | å½±å“æ¨¡å‹å®¹é‡ |
| **num_heads** | [4, 16] | 8 | é€šå¸¸ä¸º2çš„å¹‚ |
| **num_layers** | [3, 12] | 6 | æ›´æ·±ä¸ä¸€å®šæ›´å¥½ |
| **dropout** | [0.0, 0.3] | 0.1 | é˜²æ­¢è¿‡æ‹Ÿåˆ |

---

### 3.2 è¶…å‚æ•°æœç´¢ç­–ç•¥

#### ç­–ç•¥1: ç½‘æ ¼æœç´¢ï¼ˆé€‚åˆå‚æ•°å°‘ï¼‰

```java
public Map<String, Object> gridSearch() {
    double[] lrChoices = {1e-4, 1e-3, 1e-2};
    int[] batchChoices = {16, 32, 64};
    
    double bestScore = Double.NEGATIVE_INFINITY;
    Map<String, Object> bestParams = null;
    
    for (double lr : lrChoices) {
        for (int batch : batchChoices) {
            double score = trainAndEvaluate(lr, batch);
            
            if (score > bestScore) {
                bestScore = score;
                bestParams = Map.of("lr", lr, "batch", batch);
            }
        }
    }
    
    return bestParams;
}
```

#### ç­–ç•¥2: éšæœºæœç´¢ï¼ˆæ¨èï¼‰

```java
public Map<String, Object> randomSearch(int numTrials) {
    Random random = new Random();
    double bestScore = Double.NEGATIVE_INFINITY;
    Map<String, Object> bestParams = null;
    
    for (int i = 0; i < numTrials; i++) {
        // éšæœºé‡‡æ ·è¶…å‚æ•°
        double lr = Math.pow(10, -5 + random.nextDouble() * 3);
        int batch = 16 * (1 << random.nextInt(4));  // 16, 32, 64, 128
        int hiddenDim = 256 * (1 << random.nextInt(3));  // 256, 512, 1024
        
        double score = trainAndEvaluate(lr, batch, hiddenDim);
        
        if (score > bestScore) {
            bestScore = score;
            bestParams = Map.of(
                "lr", lr,
                "batch", batch,
                "hidden_dim", hiddenDim
            );
        }
    }
    
    return bestParams;
}
```

---

## 4. æ•°æ®å¤„ç†æœ€ä½³å®è·µ

### 4.1 æ•°æ®å¢å¼º

#### âœ… è§†è§‰æ•°æ®å¢å¼º

```java
public VisionInput augmentVisionInput(VisionInput original) {
    NdArray image = original.getRgbImage();
    
    // éšæœºç¿»è½¬
    if (Math.random() < 0.5) {
        image = flipHorizontal(image);
    }
    
    // éšæœºæ—‹è½¬ï¼ˆå°è§’åº¦ï¼‰
    double angle = (Math.random() - 0.5) * 20;  // -10Â° to +10Â°
    image = rotate(image, angle);
    
    // éšæœºäº®åº¦è°ƒæ•´
    double brightness = 0.8 + Math.random() * 0.4;  // 0.8 to 1.2
    image = adjustBrightness(image, brightness);
    
    // æ·»åŠ è½»å¾®å™ªå£°
    image = addGaussianNoise(image, 0.01);
    
    return new VisionInput(image);
}
```

#### âœ… è¯­è¨€æ•°æ®å¢å¼º

```java
public LanguageInput augmentLanguageInput(LanguageInput original) {
    String instruction = original.getInstruction();
    
    // åŒä¹‰è¯æ›¿æ¢
    instruction = replaceSynonyms(instruction);
    
    // å¥å¼å˜æ¢
    instruction = paraphrase(instruction);
    
    return new LanguageInput(instruction);
}
```

---

### 4.2 æ•°æ®å½’ä¸€åŒ–

#### âœ… æ¨èåšæ³•

```java
// âœ“ å½’ä¸€åŒ–è¾“å…¥æ•°æ®
public NdArray normalizeImage(NdArray image) {
    // å½’ä¸€åŒ–åˆ°[0, 1]
    return image.div(NdArray.of(255.0));
}

public NdArray normalizeAction(NdArray action) {
    // å½’ä¸€åŒ–åˆ°[-1, 1]
    double[] range = getActionRange();
    return action.sub(NdArray.of(range[0]))
                 .div(NdArray.of(range[1] - range[0]))
                 .mul(NdArray.of(2.0))
                 .sub(NdArray.of(1.0));
}
```

---

## 5. æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### 5.1 æ¨ç†åŠ é€Ÿ

#### âœ… æ‰¹å¤„ç†æ¨ç†

```java
// âœ“ æ‰¹å¤„ç†å¤šä¸ªçŠ¶æ€
public List<VLAAction> batchPredict(List<VLAState> states) {
    // å°†å¤šä¸ªçŠ¶æ€æ‰“åŒ…æˆæ‰¹æ¬¡
    NdArray batchInput = stackStates(states);
    
    // ä¸€æ¬¡å‰å‘ä¼ æ’­
    NdArray batchOutput = agent.forward(batchInput);
    
    // è§£åŒ…ç»“æœ
    return unbatchActions(batchOutput);
}
```

#### âœ… ç‰¹å¾ç¼“å­˜

```java
// âœ“ ç¼“å­˜ä¸å˜çš„ç‰¹å¾
private Map<String, NdArray> featureCache = new HashMap<>();

public NdArray encodeLanguage(String instruction) {
    // æ£€æŸ¥ç¼“å­˜
    if (featureCache.containsKey(instruction)) {
        return featureCache.get(instruction);
    }
    
    // ç¼–ç å¹¶ç¼“å­˜
    NdArray features = languageEncoder.encode(instruction);
    featureCache.put(instruction, features);
    
    return features;
}
```

---

### 5.2 å†…å­˜ä¼˜åŒ–

#### âœ… æ¢¯åº¦ç´¯ç§¯

```java
// âœ“ ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡
int accumulationSteps = 4;
int microBatchSize = 8;
// ç­‰æ•ˆæ‰¹æ¬¡å¤§å° = 4 * 8 = 32

for (int step = 0; step < accumulationSteps; step++) {
    List<StateActionPair> microBatch = getNextMicroBatch(microBatchSize);
    
    // å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼ˆä¸æ›´æ–°å‚æ•°ï¼‰
    double loss = computeLoss(microBatch);
    agent.backward(loss, retainGraph=true);
}

// ç´¯ç§¯å®Œæˆåæ‰æ›´æ–°å‚æ•°
optimizer.step();
optimizer.zeroGrad();
```

---

## 6. æ¨¡å‹è¯„ä¼°æœ€ä½³å®è·µ

### 6.1 è¯„ä¼°æŒ‡æ ‡

#### æ¨èæŒ‡æ ‡é›†

```java
public EvaluationMetrics evaluate(VLAAgent agent, RobotEnvironment env) {
    EvaluationMetrics metrics = new EvaluationMetrics();
    
    for (int i = 0; i < numEvalEpisodes; i++) {
        VLAState state = env.reset();
        double episodeReward = 0.0;
        int steps = 0;
        
        while (steps < maxSteps) {
            VLAAction action = agent.predict(state);
            RobotEnvironment.EnvironmentStep envStep = env.step(action);
            
            episodeReward += envStep.getReward();
            steps++;
            
            if (envStep.isDone()) {
                break;
            }
            
            state = envStep.getNextState();
        }
        
        // è®°å½•æŒ‡æ ‡
        metrics.addReward(episodeReward);
        metrics.addSteps(steps);
        metrics.addSuccess(episodeReward > successThreshold);
    }
    
    return metrics;
}

class EvaluationMetrics {
    // æ ¸å¿ƒæŒ‡æ ‡
    public double getAverageReward();
    public double getSuccessRate();
    public double getAverageSteps();
    
    // ç¨³å®šæ€§æŒ‡æ ‡
    public double getRewardStd();
    public double getMinReward();
    public double getMaxReward();
    
    // æ•ˆç‡æŒ‡æ ‡
    public double getAverageInferenceTime();
}
```

---

### 6.2 äº¤å‰éªŒè¯

```java
// âœ“ KæŠ˜äº¤å‰éªŒè¯
public double kFoldValidation(int k) {
    List<Double> scores = new ArrayList<>();
    
    for (int fold = 0; fold < k; fold++) {
        // åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        Pair<List<Data>, List<Data>> split = splitData(fold, k);
        
        // åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒ
        VLAAgent agent = trainOnData(split.getFirst());
        
        // åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        double score = evaluateOnData(agent, split.getSecond());
        scores.add(score);
    }
    
    // è¿”å›å¹³å‡åˆ†æ•°
    return scores.stream().mapToDouble(Double::doubleValue).average().orElse(0.0);
}
```

---

## 7. ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ

### 7.1 æ¨¡å‹å¯¼å‡º

```java
// âœ“ å¯¼å‡ºä¸ºæ ‡å‡†æ ¼å¼
public void exportModel(VLAAgent agent, String path) {
    // ä¿å­˜æ¨¡å‹å‚æ•°
    agent.saveParameters(path + "/parameters.bin");
    
    // ä¿å­˜æ¨¡å‹é…ç½®
    ModelConfig config = agent.getConfig();
    config.saveToJson(path + "/config.json");
    
    // ä¿å­˜å½’ä¸€åŒ–ç»Ÿè®¡é‡
    saveNormalizationStats(path + "/norm_stats.json");
    
    // ä¿å­˜è¯è¡¨
    saveVocabulary(path + "/vocabulary.txt");
}
```

---

### 7.2 æ¨ç†æœåŠ¡

```java
// âœ“ æ„å»ºæ¨ç†æœåŠ¡
public class VLAInferenceService {
    private VLAAgent agent;
    private ExecutorService executor;
    
    public VLAInferenceService(String modelPath) {
        // åŠ è½½æ¨¡å‹
        this.agent = VLAAgent.load(modelPath);
        
        // åˆ›å»ºçº¿ç¨‹æ± 
        this.executor = Executors.newFixedThreadPool(4);
    }
    
    public CompletableFuture<VLAAction> predictAsync(VLAState state) {
        return CompletableFuture.supplyAsync(
            () -> agent.predict(state),
            executor
        );
    }
    
    public List<VLAAction> batchPredict(List<VLAState> states) {
        return agent.batchPredict(states);
    }
}
```

---

## 8. å¸¸è§é™·é˜±é¿å…

### 8.1 è®­ç»ƒé™·é˜±

#### âŒ é™·é˜±1: å¥–åŠ±é»‘å®¢

**é—®é¢˜æè¿°**:
```java
// âœ— æ™ºèƒ½ä½“æ‰¾åˆ°äº†æ„å¤–çš„é«˜å¥–åŠ±è¡Œä¸º
// ä¾‹å¦‚ï¼šæœºå™¨äººä¸€ç›´åœ¨åŸåœ°æŠ–åŠ¨è·å–"è¿åŠ¨"å¥–åŠ±
```

**è§£å†³æ–¹æ¡ˆ**:
```java
// âœ“ è®¾è®¡æ›´åˆç†çš„å¥–åŠ±å‡½æ•°
double reward = 0.0;

// è·ç¦»å¥–åŠ±ï¼ˆé€’å‡ï¼‰
double distReward = Math.max(0, prevDistance - currentDistance);
reward += distReward;

// æƒ©ç½šè¿‡åº¦è¿åŠ¨
double movementPenalty = 0.01 * actionMagnitude;
reward -= movementPenalty;

// æˆåŠŸå¥–åŠ±
if (taskCompleted) {
    reward += 100.0;
}
```

---

#### âŒ é™·é˜±2: è¿‡æ‹Ÿåˆè®­ç»ƒç¯å¢ƒ

**é—®é¢˜æè¿°**:
```java
// âœ— æ¨¡å‹åœ¨å›ºå®šåœºæ™¯ä¸Šè¿‡æ‹Ÿåˆ
// è®­ç»ƒé›†ï¼š90%æˆåŠŸç‡
// æµ‹è¯•é›†ï¼š20%æˆåŠŸç‡
```

**è§£å†³æ–¹æ¡ˆ**:
```java
// âœ“ å¢åŠ ç¯å¢ƒéšæœºæ€§
public VLAState reset() {
    // éšæœºåˆå§‹çŠ¶æ€
    randomizeObjectPositions();
    randomizeObjectProperties();
    randomizeLighting();
    randomizeBackgroundTexture();
    
    return createState();
}
```

---

#### âŒ é™·é˜±3: å­¦ä¹ ç‡è¿‡å¤§

**é—®é¢˜æè¿°**:
```java
// âœ— æŸå¤±éœ‡è¡ï¼Œæ— æ³•æ”¶æ•›
// Loss: 10.5 -> 8.2 -> 12.1 -> 9.8 -> 15.3 ...
```

**è§£å†³æ–¹æ¡ˆ**:
```java
// âœ“ ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡æˆ–é™ä½åˆå§‹å­¦ä¹ ç‡
// æ–¹æ¡ˆ1: é™ä½åˆå§‹å­¦ä¹ ç‡
double learningRate = 0.0001;  // ä»0.001é™ä½åˆ°0.0001

// æ–¹æ¡ˆ2: ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦
double lr = getCosineLR(episode, maxEpisodes, 0.001, 0.00001);
```

---

### 8.2 è¯„ä¼°é™·é˜±

#### âŒ é™·é˜±4: åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°

**é—®é¢˜æè¿°**:
```java
// âœ— åœ¨è®­ç»ƒè¿‡çš„åœºæ™¯ä¸Šæµ‹è¯•
double reward = evaluate(agent, trainingEnv);
// è¿‡äºä¹è§‚çš„æ€§èƒ½ä¼°è®¡
```

**è§£å†³æ–¹æ¡ˆ**:
```java
// âœ“ ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•é›†
RobotEnvironment testEnv = createNewEnvironment();
testEnv.setRandomSeed(12345);  // ä¸åŒäºè®­ç»ƒæ—¶çš„ç§å­

double reward = evaluate(agent, testEnv);
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†å‚è€ƒ

### æ¨èæ€§èƒ½ç›®æ ‡

| ä»»åŠ¡ | æˆåŠŸç‡ | å¹³å‡å¥–åŠ± | å¹³å‡æ­¥æ•° | æ¨ç†å»¶è¿Ÿ |
|------|--------|---------|---------|---------|
| **PickAndPlace** | >85% | >80 | <50 | <50ms |
| **StackBlocks (2)** | >90% | >180 | <60 | <50ms |
| **StackBlocks (3)** | >70% | >250 | <80 | <50ms |
| **StackBlocks (4)** | >50% | >300 | <100 | <50ms |
| **OpenDrawer** | >80% | >120 | <70 | <50ms |

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Behavior Cloning è®ºæ–‡](https://arxiv.org/abs/1406.2661)
- [Curriculum Learning è®ºæ–‡](https://arxiv.org/abs/1904.04912)
- [Vision-Language-Action Models ç»¼è¿°](https://arxiv.org/abs/2307.15818)

---

**ç‰ˆæœ¬**: v1.0  
**æ›´æ–°æ—¶é—´**: 2025-10-18  
**ç»´æŠ¤è€…**: TinyAI VLA Team

*æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿åé¦ˆå’Œå»ºè®®ï¼*
