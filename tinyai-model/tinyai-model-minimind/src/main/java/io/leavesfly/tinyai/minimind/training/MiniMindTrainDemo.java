package io.leavesfly.tinyai.minimind.training;

import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.minimind.model.MiniMindConfig;
import io.leavesfly.tinyai.minimind.model.MiniMindModel;
import io.leavesfly.tinyai.minimind.tokenizer.MiniMindTokenizer;
import io.leavesfly.tinyai.minimind.training.dataset.PretrainDataset;
import io.leavesfly.tinyai.minimind.training.dataset.SFTDataset;
import io.leavesfly.tinyai.ml.loss.SoftmaxCrossEntropy;
import io.leavesfly.tinyai.ml.optimize.Adam;
import io.leavesfly.tinyai.ndarr.NdArray;
import io.leavesfly.tinyai.ndarr.Shape;

import java.io.*;
import java.util.*;

/**
 * MiniMind å®Œæ•´è®­ç»ƒæ¼”ç¤º
 * <p>
 * å‚è€ƒ DeepSeekV3TrainDemoV2 çš„å®ç°æ–¹å¼ï¼Œæä¾›å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼š
 * 1. å‡†å¤‡çœŸå®çš„æ•™å­¦æ•°æ®é›†ï¼ˆé€‚ç”¨äºæ•™è‚²å­¦ä¹ ï¼‰
 * 2. é¢„è®­ç»ƒé˜¶æ®µ - æ— ç›‘ç£è¯­è¨€å»ºæ¨¡è®­ç»ƒ
 * 3. å¾®è°ƒé˜¶æ®µ - ç›‘ç£æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰
 * 4. å¼ºåŒ–å­¦ä¹ é˜¶æ®µ - RLAIFè®­ç»ƒ
 * 5. æ¨ç†é˜¶æ®µ - å¤šç§ç”Ÿæˆç­–ç•¥æ¼”ç¤º
 * <p>
 * æ•°æ®é›†ç‰¹ç‚¹ï¼š
 * - è¶…å°è§„æ¨¡ï¼Œä¾¿äºå¿«é€Ÿæ‰§è¡Œ
 * - å†…å®¹æ¸…æ™°ï¼Œé€‚åˆæ•™å­¦æ¼”ç¤º
 * - è¦†ç›–å®Œæ•´è®­ç»ƒæµç¨‹
 *
 * @author TinyAI Team
 * @version 1.0
 */
public class MiniMindTrainDemo {

    /** å…±äº«åˆ†è¯å™¨ - ä½¿ç”¨æ ‡å‡† MiniMindTokenizer */
    private static MiniMindTokenizer sharedTokenizer;

    private static final String DATA_DIR = "./data/minimind_training";
    private static final String CHECKPOINT_DIR = "./checkpoints/minimind";

    public static void main(String[] args) {
        System.out.println("=".repeat(80));
        System.out.println("MiniMind å®Œæ•´è®­ç»ƒä¸æ¨ç†æ¼”ç¤º");
        System.out.println("é€‚ç”¨äºæ•™å­¦å’Œå­¦ä¹ çš„è¶…å°è§„æ¨¡æ•°æ®é›†è®­ç»ƒæ–¹æ¡ˆ");
        System.out.println("=".repeat(80));

        try {
            // æ­¥éª¤0: å‡†å¤‡æ•°æ®é›†æ–‡ä»¶
            prepareDatasets();

            // æ­¥éª¤1: æ— ç›‘ç£é¢„è®­ç»ƒ
            MiniMindModel pretrainedModel = runUnsupervisedPretraining();

            // æ­¥éª¤2: ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰
            MiniMindModel finetunedModel = runSupervisedFinetuning(pretrainedModel);

            // æ­¥éª¤3: å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ˆRLAIFï¼‰
            MiniMindModel rlModel = runReinforcementLearningTraining(finetunedModel);

            // æ­¥éª¤4: æ¨ç†æµ‹è¯•
            runInference(rlModel);

            System.out.println("\n" + "=".repeat(80));
            System.out.println("âœ… å®Œæ•´è®­ç»ƒæµç¨‹æ¼”ç¤ºæˆåŠŸ!");
            System.out.println("=".repeat(80));

        } catch (Exception e) {
            System.err.println("âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: " + e.getMessage());
            e.printStackTrace();
        }
    }

    // ========== æ­¥éª¤0: æ•°æ®å‡†å¤‡ ==========

    /**
     * å‡†å¤‡è®­ç»ƒæ•°æ®é›†
     */
    private static void prepareDatasets() throws IOException {
        System.out.println("\n" + "=".repeat(80));
        System.out.println("ğŸ“¦ æ­¥éª¤0: å‡†å¤‡è®­ç»ƒæ•°æ®é›†");
        System.out.println("=".repeat(80));

        File dataDir = new File(DATA_DIR);
        if (!dataDir.exists()) {
            dataDir.mkdirs();
            System.out.println("âœ“ åˆ›å»ºæ•°æ®ç›®å½•: " + DATA_DIR);
        }

        // ç”Ÿæˆé¢„è®­ç»ƒæ•°æ®é›†
        generatePretrainDataset();

        // ç”Ÿæˆç›‘ç£å¾®è°ƒæ•°æ®é›†
        generateSFTDataset();

        // ç”Ÿæˆå¼ºåŒ–å­¦ä¹ æ•°æ®é›†
        generateRLDataset();

        System.out.println("\nâœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ!");
    }

    /**
     * ç”Ÿæˆé¢„è®­ç»ƒæ•°æ®é›†
     * åŒ…å«é€šç”¨è¯­è¨€çŸ¥è¯†
     */
    private static void generatePretrainDataset() throws IOException {
        System.out.println("\nğŸ“ ç”Ÿæˆé¢„è®­ç»ƒæ•°æ®é›†...");

        List<String> pretrainTexts = new ArrayList<>();

        // 1. æ·±åº¦å­¦ä¹ åŸºç¡€çŸ¥è¯† (30æ¡)
        pretrainTexts.addAll(Arrays.asList(
            "Deep learning is a subset of machine learning that uses neural networks",
            "Neural networks consist of interconnected layers of neurons",
            "Backpropagation is the algorithm used to train neural networks",
            "Gradient descent optimizes neural network parameters",
            "Activation functions introduce non-linearity into neural networks",
            "Convolutional neural networks excel at image processing tasks",
            "Recurrent neural networks process sequential data effectively",
            "Transformer architecture revolutionized natural language processing",
            "Attention mechanism allows models to focus on relevant information",
            "Pre-training followed by fine-tuning is a common training strategy",
            "Overfitting occurs when a model memorizes training data",
            "Regularization techniques prevent overfitting in neural networks",
            "Dropout randomly disables neurons during training",
            "Batch normalization stabilizes training of deep networks",
            "Learning rate controls the speed of gradient descent",
            "Adam optimizer adapts learning rates for each parameter",
            "Loss function measures the difference between prediction and truth",
            "Cross-entropy loss is commonly used for classification",
            "Mean squared error is used for regression problems",
            "Early stopping prevents overfitting by monitoring validation loss",
            "Data augmentation increases training data diversity",
            "Transfer learning reuses pre-trained models for new tasks",
            "Embedding layers convert discrete tokens into continuous vectors",
            "Positional encoding adds position information to embeddings",
            "Multi-head attention processes information in parallel",
            "Feedforward networks transform attention outputs",
            "Layer normalization normalizes activations across features",
            "Residual connections help gradients flow through deep networks",
            "Softmax function converts logits to probabilities",
            "Tokenization splits text into meaningful units"
        ));

        // 2. è‡ªç„¶è¯­è¨€å¤„ç†çŸ¥è¯† (30æ¡)
        pretrainTexts.addAll(Arrays.asList(
            "Language models predict the next word in a sequence",
            "Autoregressive models generate text one token at a time",
            "BERT uses bidirectional context for understanding",
            "GPT models use unidirectional context for generation",
            "Fine-tuning adapts pre-trained models to specific tasks",
            "Text classification assigns categories to documents",
            "Named entity recognition identifies entities in text",
            "Sentiment analysis determines emotional tone of text",
            "Machine translation converts text between languages",
            "Question answering systems extract answers from context",
            "Summarization condenses long text into key points",
            "Text generation creates coherent natural language",
            "Perplexity measures language model quality",
            "BLEU score evaluates machine translation quality",
            "Word embeddings capture semantic relationships",
            "Byte-pair encoding handles rare words effectively",
            "Subword tokenization balances vocabulary size and coverage",
            "Masked language modeling is used in BERT pre-training",
            "Causal language modeling is used in GPT pre-training",
            "Few-shot learning enables models to learn from examples",
            "Zero-shot learning performs tasks without specific training",
            "Prompt engineering guides model behavior through input design",
            "In-context learning uses examples within the prompt",
            "Instruction tuning teaches models to follow commands",
            "Reinforcement learning from human feedback aligns models",
            "Temperature controls randomness in text generation",
            "Top-k sampling limits choices to k most probable tokens",
            "Top-p sampling uses cumulative probability threshold",
            "Beam search explores multiple generation paths",
            "Greedy decoding always selects the most probable token"
        ));

        // 3. æœºå™¨å­¦ä¹ æ¦‚å¿µ (30æ¡)
        pretrainTexts.addAll(Arrays.asList(
            "Supervised learning uses labeled data for training",
            "Unsupervised learning finds patterns without labels",
            "Reinforcement learning learns through rewards and penalties",
            "Classification predicts discrete categories",
            "Regression predicts continuous values",
            "Clustering groups similar data points together",
            "Dimensionality reduction simplifies high-dimensional data",
            "Feature engineering creates informative input variables",
            "Cross-validation assesses model generalization",
            "Train-test split separates data for training and evaluation",
            "Validation set helps tune hyperparameters",
            "Precision measures positive prediction accuracy",
            "Recall measures coverage of actual positives",
            "F1 score balances precision and recall",
            "Accuracy measures overall prediction correctness",
            "Confusion matrix visualizes classification performance",
            "ROC curve plots true positive versus false positive rates",
            "AUC measures area under ROC curve",
            "Bias-variance tradeoff affects model performance",
            "Ensemble methods combine multiple models",
            "Bagging reduces variance through averaging",
            "Boosting sequentially improves weak learners",
            "Random forest uses ensemble of decision trees",
            "Gradient boosting builds trees to correct errors",
            "Neural architecture search automates model design",
            "Hyperparameter tuning optimizes model configuration",
            "Grid search exhaustively tries parameter combinations",
            "Random search samples parameter space randomly",
            "Bayesian optimization uses probabilistic models",
            "Meta-learning enables learning to learn"
        ));

        // 4. AIä¼¦ç†ä¸åº”ç”¨ (30æ¡)
        pretrainTexts.addAll(Arrays.asList(
            "Artificial intelligence transforms many industries",
            "AI ethics ensures responsible development",
            "Fairness in AI prevents discrimination",
            "Bias in training data leads to biased models",
            "Transparency makes AI decisions interpretable",
            "Explainable AI helps humans understand model reasoning",
            "Privacy protection is crucial in AI systems",
            "Data security prevents unauthorized access",
            "AI safety ensures systems behave as intended",
            "Robustness makes models resilient to attacks",
            "Adversarial examples fool neural networks",
            "Model interpretability reveals decision factors",
            "Feature importance shows influential variables",
            "Attention visualization reveals focus areas",
            "Counterfactual explanations show decision boundaries",
            "AI applications include healthcare diagnostics",
            "Computer vision enables autonomous vehicles",
            "Natural language processing powers virtual assistants",
            "Recommendation systems personalize user experiences",
            "Fraud detection identifies suspicious transactions",
            "Predictive maintenance prevents equipment failures",
            "Drug discovery accelerates pharmaceutical research",
            "Climate modeling predicts environmental changes",
            "Robotics combines AI with physical systems",
            "Speech recognition converts audio to text",
            "Image generation creates realistic visuals",
            "Style transfer applies artistic styles to images",
            "Anomaly detection identifies unusual patterns",
            "Time series forecasting predicts future values",
            "Knowledge graphs organize structured information"
        ));

        // 5. ç¼–ç¨‹ä¸è½¯ä»¶å¼€å‘ (30æ¡)
        pretrainTexts.addAll(Arrays.asList(
            "Programming languages enable human-computer communication",
            "Python is popular for machine learning development",
            "Java offers robust object-oriented programming",
            "JavaScript powers interactive web applications",
            "Data structures organize and store information",
            "Algorithms solve computational problems efficiently",
            "Version control tracks code changes over time",
            "Git is widely used for version control",
            "Code review improves software quality",
            "Unit testing verifies individual components",
            "Integration testing checks component interactions",
            "Continuous integration automates testing",
            "Software design patterns solve common problems",
            "Object-oriented programming uses classes and objects",
            "Functional programming emphasizes pure functions",
            "Debugging identifies and fixes code errors",
            "Profiling measures code performance",
            "Optimization improves execution speed",
            "Memory management prevents resource leaks",
            "Exception handling manages runtime errors",
            "API design defines software interfaces",
            "Documentation explains code functionality",
            "Code refactoring improves code structure",
            "Modularity breaks code into manageable pieces",
            "Abstraction hides implementation details",
            "Encapsulation bundles data with methods",
            "Inheritance enables code reuse",
            "Polymorphism allows flexible implementations",
            "Dependency injection improves testability",
            "Clean code principles enhance readability"
        ));

        // å†™å…¥æ–‡ä»¶
        String filePath = DATA_DIR + "/pretrain.txt";
        writeToFile(pretrainTexts, filePath);

        System.out.println("  âœ“ é¢„è®­ç»ƒæ•°æ®: " + pretrainTexts.size() + " æ¡");
        System.out.println("  âœ“ ä¿å­˜è·¯å¾„: " + filePath);
    }

    /**
     * ç”Ÿæˆç›‘ç£å¾®è°ƒæ•°æ®é›†
     * åŒ…å«æŒ‡ä»¤-å›ç­”å¯¹
     */
    private static void generateSFTDataset() throws IOException {
        System.out.println("\nğŸ“ ç”Ÿæˆç›‘ç£å¾®è°ƒæ•°æ®é›†...");

        List<String> sftTrainTexts = new ArrayList<>();
        List<String> sftValTexts = new ArrayList<>();

        // è®­ç»ƒé›†: 60æ¡æŒ‡ä»¤-å›ç­”å¯¹
        sftTrainTexts.addAll(Arrays.asList(
            "Question: What is deep learning? Answer: Deep learning is a subset of machine learning using neural networks with multiple layers",
            "Question: Explain backpropagation Answer: Backpropagation is an algorithm that computes gradients to update neural network weights",
            "Question: What is overfitting? Answer: Overfitting occurs when a model memorizes training data instead of learning general patterns",
            "Question: Define gradient descent Answer: Gradient descent is an optimization algorithm that minimizes loss by updating parameters",
            "Question: What are transformers? Answer: Transformers are neural network architectures using self-attention mechanisms",
            "Question: Explain attention mechanism Answer: Attention allows models to focus on relevant parts of input when processing information",
            "Question: What is fine-tuning? Answer: Fine-tuning adapts pre-trained models to specific tasks with additional training",
            "Question: Define reinforcement learning Answer: Reinforcement learning trains agents through rewards and penalties for actions",
            "Question: What is tokenization? Answer: Tokenization splits text into smaller units like words or subwords for processing",
            "Question: Explain embedding layers Answer: Embedding layers convert discrete tokens into continuous vector representations",
            "Instruction: Write a Python function to add two numbers Answer: def add(a, b): return a + b",
            "Instruction: Create a loop to print numbers 1 to 5 Answer: for i in range(1, 6): print(i)",
            "Instruction: Define a class for a person Answer: class Person: def __init__(self, name): self.name = name",
            "Instruction: Implement binary search Answer: Binary search finds elements in sorted arrays by dividing search space",
            "Instruction: Explain list comprehension Answer: List comprehension creates lists using concise syntax: [x*2 for x in range(10)]",
            "Task: Summarize this concept: Neural networks Answer: Networks of artificial neurons that learn patterns from data",
            "Task: Classify this as positive or negative: I love this product Answer: Positive sentiment",
            "Task: Translate to simple terms: Convolutional neural network Answer: Network specialized for processing grid-like data such as images",
            "Task: Generate a creative name for an AI assistant Answer: MindBot - your intelligent companion",
            "Task: Suggest improvements for code readability Answer: Use meaningful variable names and add comments",
            "Question: How does BERT work? Answer: BERT uses bidirectional transformers to understand context from both directions",
            "Question: What is GPT? Answer: GPT is a generative pre-trained transformer for autoregressive language modeling",
            "Question: Explain cross-entropy loss Answer: Cross-entropy measures difference between predicted and true probability distributions",
            "Question: What is batch normalization? Answer: Batch normalization normalizes layer inputs to stabilize and speed up training",
            "Question: Define learning rate Answer: Learning rate controls step size in gradient descent optimization",
            "Instruction: Sort a list in Python Answer: sorted_list = sorted(my_list) or my_list.sort()",
            "Instruction: Handle exceptions in Python Answer: try: risky_code() except Exception as e: handle_error(e)",
            "Instruction: Read a file in Python Answer: with open('file.txt', 'r') as f: content = f.read()",
            "Instruction: Create a dictionary Answer: my_dict = {'key1': 'value1', 'key2': 'value2'}",
            "Instruction: Use list slicing Answer: first_three = my_list[:3], last_two = my_list[-2:]",
            "Task: Explain AI ethics Answer: AI ethics ensures responsible development considering fairness bias and transparency",
            "Task: Compare supervised and unsupervised learning Answer: Supervised uses labels unsupervised finds patterns without labels",
            "Task: Recommend a machine learning algorithm Answer: For classification try random forest or neural networks",
            "Task: Debug this error: IndexError Answer: Check array bounds and ensure index is within valid range",
            "Task: Optimize slow code Answer: Profile to find bottlenecks use efficient algorithms and data structures",
            "Question: What is transfer learning? Answer: Transfer learning reuses pre-trained models for new related tasks",
            "Question: Explain dropout regularization Answer: Dropout randomly disables neurons during training to prevent overfitting",
            "Question: What is a loss function? Answer: Loss function quantifies difference between model predictions and true values",
            "Question: Define activation functions Answer: Activation functions introduce non-linearity enabling networks to learn complex patterns",
            "Question: What is early stopping? Answer: Early stopping halts training when validation performance stops improving",
            "Instruction: Import libraries in Python Answer: import numpy as np, import pandas as pd",
            "Instruction: Create a virtual environment Answer: python -m venv myenv, source myenv/bin/activate",
            "Instruction: Install packages Answer: pip install package_name",
            "Instruction: Format strings in Python Answer: f'Hello {name}' or 'Hello {}'.format(name)",
            "Instruction: Use lambda functions Answer: square = lambda x: x**2",
            "Task: Improve model accuracy Answer: Try feature engineering data augmentation or ensemble methods",
            "Task: Reduce training time Answer: Use smaller batches GPU acceleration or model pruning",
            "Task: Prevent data leakage Answer: Split data before preprocessing keep test set completely separate",
            "Task: Handle imbalanced data Answer: Use oversampling undersampling or class weights",
            "Task: Validate model performance Answer: Use cross-validation and multiple metrics",
            "Question: What is ensemble learning? Answer: Ensemble learning combines multiple models to improve predictions",
            "Question: Explain feature engineering Answer: Feature engineering creates informative variables from raw data",
            "Question: What is regularization? Answer: Regularization adds penalties to prevent overfitting and improve generalization",
            "Question: Define precision and recall Answer: Precision is accuracy of positive predictions recall is coverage of actual positives",
            "Question: What is the bias-variance tradeoff? Answer: Balancing model complexity to minimize both underfitting and overfitting",
            "Instruction: Use NumPy arrays Answer: import numpy as np, arr = np.array([1, 2, 3])",
            "Instruction: Plot data with Matplotlib Answer: import matplotlib.pyplot as plt, plt.plot(x, y), plt.show()",
            "Instruction: Create pandas DataFrame Answer: import pandas as pd, df = pd.DataFrame(data)",
            "Instruction: Apply function to DataFrame Answer: df['new_col'] = df['col'].apply(lambda x: x*2)",
            "Instruction: Split train-test data Answer: from sklearn.model_selection import train_test_split"
        ));

        // éªŒè¯é›†: ä»è®­ç»ƒé›†ä¸­æŠ½å–10æ¡
        for (int i = 0; i < 10 && i < sftTrainTexts.size(); i++) {
            sftValTexts.add(sftTrainTexts.get(i));
        }

        // å†™å…¥è®­ç»ƒé›†
        String trainPath = DATA_DIR + "/sft_train.txt";
        writeToFile(sftTrainTexts, trainPath);
        System.out.println("  âœ“ SFTè®­ç»ƒé›†: " + sftTrainTexts.size() + " æ¡");
        System.out.println("  âœ“ ä¿å­˜è·¯å¾„: " + trainPath);

        // å†™å…¥éªŒè¯é›†
        String valPath = DATA_DIR + "/sft_val.txt";
        writeToFile(sftValTexts, valPath);
        System.out.println("  âœ“ SFTéªŒè¯é›†: " + sftValTexts.size() + " æ¡");
        System.out.println("  âœ“ ä¿å­˜è·¯å¾„: " + valPath);
    }

    /**
     * ç”Ÿæˆå¼ºåŒ–å­¦ä¹ æ•°æ®é›†
     * åŒ…å«å¸¦å¥–åŠ±çš„æ ·æœ¬
     */
    private static void generateRLDataset() throws IOException {
        System.out.println("\nğŸ“ ç”Ÿæˆå¼ºåŒ–å­¦ä¹ æ•°æ®é›†...");

        List<String> rlTexts = new ArrayList<>();

        // 40æ¡å¸¦å¥–åŠ±æ ‡ç­¾çš„æ ·æœ¬
        rlTexts.addAll(Arrays.asList(
            "[REWARD:1.0] Question: What is machine learning? Answer: Machine learning enables computers to learn from data without explicit programming",
            "[REWARD:0.9] Question: Explain neural networks Answer: Neural networks are computing systems inspired by biological brains",
            "[REWARD:0.8] Question: What is deep learning? Answer: Deep learning uses multi-layer neural networks for complex pattern recognition",
            "[REWARD:1.0] Instruction: Write clean code Answer: Use meaningful names add comments and follow style guidelines",
            "[REWARD:0.9] Instruction: Debug efficiently Answer: Use print statements debuggers and unit tests",
            "[REWARD:0.7] Task: Improve performance Answer: Optimize algorithms and use better data structures",
            "[REWARD:0.8] Task: Ensure code quality Answer: Write tests review code and refactor regularly",
            "[REWARD:1.0] Question: What is AI safety? Answer: AI safety ensures systems behave reliably and aligned with human values",
            "[REWARD:0.9] Question: Define model interpretability Answer: Interpretability makes model decisions understandable to humans",
            "[REWARD:0.8] Question: What is fairness in AI? Answer: Fairness prevents discrimination and ensures equitable treatment",
            "[REWARD:1.0] Instruction: Handle errors gracefully Answer: Use try-except blocks and provide informative error messages",
            "[REWARD:0.9] Instruction: Write efficient code Answer: Avoid unnecessary loops and use vectorized operations",
            "[REWARD:0.8] Task: Document your code Answer: Write clear docstrings and maintain README files",
            "[REWARD:0.7] Task: Test thoroughly Answer: Cover edge cases and use both unit and integration tests",
            "[REWARD:1.0] Question: What is gradient descent? Answer: Gradient descent iteratively updates parameters to minimize loss",
            "[REWARD:0.9] Question: Explain overfitting prevention Answer: Use regularization dropout and cross-validation",
            "[REWARD:0.8] Question: What is transfer learning? Answer: Transfer learning applies knowledge from one task to another",
            "[REWARD:1.0] Instruction: Optimize hyperparameters Answer: Use grid search random search or Bayesian optimization",
            "[REWARD:0.9] Instruction: Prevent data leakage Answer: Split data properly and avoid using test information",
            "[REWARD:0.8] Task: Improve model robustness Answer: Use data augmentation and adversarial training",
            "[REWARD:0.7] Task: Monitor model performance Answer: Track metrics and set up alerts for degradation",
            "[REWARD:1.0] Question: What is attention mechanism? Answer: Attention helps models focus on relevant input parts",
            "[REWARD:0.9] Question: Explain transformer architecture Answer: Transformers use self-attention for parallel processing",
            "[REWARD:0.8] Question: What is BERT? Answer: BERT uses bidirectional transformers for language understanding",
            "[REWARD:1.0] Instruction: Design scalable systems Answer: Use modular architecture and efficient algorithms",
            "[REWARD:0.9] Instruction: Ensure reproducibility Answer: Set random seeds and document all parameters",
            "[REWARD:0.8] Task: Validate assumptions Answer: Check data distributions and verify preprocessing steps",
            "[REWARD:0.7] Task: Communicate results Answer: Use visualizations and explain in simple terms",
            "[REWARD:1.0] Question: What is fine-tuning? Answer: Fine-tuning adapts pre-trained models to specific tasks",
            "[REWARD:0.9] Question: Explain data augmentation Answer: Data augmentation increases diversity by transforming existing data",
            "[REWARD:0.8] Question: What is batch normalization? Answer: Batch normalization normalizes inputs to stabilize training",
            "[REWARD:1.0] Instruction: Write modular code Answer: Break complex functions into smaller reusable components",
            "[REWARD:0.9] Instruction: Follow best practices Answer: Use version control write tests and review code",
            "[REWARD:0.8] Task: Optimize memory usage Answer: Use generators avoid copying and release resources",
            "[REWARD:0.7] Task: Profile code performance Answer: Identify bottlenecks and optimize critical paths",
            "[REWARD:1.0] Question: What is ensemble learning? Answer: Ensemble learning combines multiple models for better predictions",
            "[REWARD:0.9] Question: Explain cross-validation Answer: Cross-validation assesses model performance on multiple data splits",
            "[REWARD:0.8] Question: What is feature engineering? Answer: Feature engineering creates informative variables from raw data",
            "[REWARD:1.0] Instruction: Handle edge cases Answer: Test boundary conditions and null inputs",
            "[REWARD:0.9] Task: Maintain code quality Answer: Refactor regularly and eliminate technical debt"
        ));

        // å†™å…¥æ–‡ä»¶
        String filePath = DATA_DIR + "/rl_train.txt";
        writeToFile(rlTexts, filePath);

        System.out.println("  âœ“ RLè®­ç»ƒæ•°æ®: " + rlTexts.size() + " æ¡");
        System.out.println("  âœ“ ä¿å­˜è·¯å¾„: " + filePath);
    }

    // ========== æ­¥éª¤1: æ— ç›‘ç£é¢„è®­ç»ƒ ==========

    /**
     * æ‰§è¡Œæ— ç›‘ç£é¢„è®­ç»ƒ - ä½¿ç”¨æ ‡å‡† PretrainTrainer
     */
    private static MiniMindModel runUnsupervisedPretraining() throws IOException {
        System.out.println("\n" + "=".repeat(80));
        System.out.println("ğŸ“š æ­¥éª¤1: MiniMind æ— ç›‘ç£é¢„è®­ç»ƒ (Unsupervised Pretraining)");
        System.out.println("=".repeat(80));

        // 1. åˆ›å»ºå­—ç¬¦çº§åˆ†è¯å™¨ï¼ˆç”¨äºæ•™å­¦æ¼”ç¤ºï¼‰
        System.out.println("\nğŸ“ åˆ›å»ºåˆ†è¯å™¨...");
        int vocabSize = 1024;  // è¶³å¤Ÿè¦†ç›–æ•™å­¦æ•°æ®é›†
        int maxSeqLen = 64;    // åºåˆ—é•¿åº¦è¦è¶³å¤Ÿå®¹çº³è®­ç»ƒæ ·æœ¬
        sharedTokenizer = MiniMindTokenizer.createCharLevelTokenizer(vocabSize, maxSeqLen);
        System.out.println("  âœ“ åˆ†è¯å™¨ç±»å‹: å­—ç¬¦çº§ (Char-Level)");
        System.out.println("  âœ“ è¯æ±‡è¡¨å¤§å°: " + sharedTokenizer.getVocabulary().getVocabSize());

        // 2. åˆ›å»ºMiniMindæ¨¡å‹ï¼ˆè¶…å°é…ç½®ï¼‰
        System.out.println("\nğŸ“ åˆ›å»ºMiniMindæ¨¡å‹...");
        MiniMindConfig config = createMicroConfig(sharedTokenizer.getVocabulary().getVocabSize());
        MiniMindModel model = new MiniMindModel("minimind-pretrain", config);

        System.out.println("  âœ“ æ¨¡å‹é…ç½®: Micro (æ•™å­¦ä¸“ç”¨)");
        System.out.println("  âœ“ è¯æ±‡è¡¨å¤§å°: " + config.getVocabSize());
        System.out.println("  âœ“ éšè—ç»´åº¦: " + config.getHiddenSize());
        System.out.println("  âœ“ å±‚æ•°: " + config.getNumLayers());
        System.out.println("  âœ“ æ³¨æ„åŠ›å¤´æ•°: " + config.getNumHeads());
        System.out.println("  âœ“ æœ€å¤§åºåˆ—é•¿åº¦: " + config.getMaxSeqLen());

        // 3. ä½¿ç”¨æ ‡å‡† PretrainDataset åŠ è½½æ•°æ®
        System.out.println("\nğŸ“ å‡†å¤‡é¢„è®­ç»ƒæ•°æ®é›†...");
        String pretrainPath = DATA_DIR + "/pretrain.txt";
        List<String> pretrainTexts = readFromFile(pretrainPath);
        
        int batchSize = 2;  // å°æ‰¹æ¬¡ä¾¿äºæ•™å­¦
        PretrainDataset dataset = new PretrainDataset(sharedTokenizer, maxSeqLen, batchSize);
        dataset.loadFromTexts(pretrainTexts);
        dataset.prepare(true);
        System.out.println("  âœ“ é¢„è®­ç»ƒæ ·æœ¬æ•°: " + dataset.getSampleCount());
        System.out.println("  âœ“ æ‰¹æ¬¡æ•°é‡: " + dataset.getBatchCount());

        // 4. ä½¿ç”¨æ ‡å‡† PretrainTrainer è¿›è¡Œè®­ç»ƒ
        System.out.println("\nğŸ“ å¼€å§‹æ— ç›‘ç£é¢„è®­ç»ƒ...");
        System.out.println("  - è®­ç»ƒç›®æ ‡: å› æœè¯­è¨€å»ºæ¨¡ (ä¸‹ä¸€ä¸ªè¯é¢„æµ‹)");
        System.out.println("  - å­¦ä¹ ç‡: 1e-2");
        System.out.println("  - è®­ç»ƒè½®æ¬¡: 3 epochs");
        System.out.println("-".repeat(80));

        PretrainTrainer trainer = new PretrainTrainer(model, dataset);
        trainer.configure(3, 1e-2f, 0, 1.0f);  // 3 epochs, lr=1e-2, no warmup
        trainer.setLogInterval(10);  // æ¯10æ­¥æ‰“å°ä¸€æ¬¡
        trainer.train();

        System.out.println("-".repeat(80));
        System.out.println("\nâœ… æ— ç›‘ç£é¢„è®­ç»ƒå®Œæˆ!");
        System.out.println("\nğŸ’¡ é¢„è®­ç»ƒé˜¶æ®µæ€»ç»“:");
        System.out.println("  - ç›®æ ‡: å­¦ä¹ è¯­è¨€çš„é€šç”¨è¡¨ç¤ºå’Œè¯­æ³•");
        System.out.println("  - ä»»åŠ¡: å› æœè¯­è¨€å»ºæ¨¡ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰");
        System.out.println("  - æ•°æ®: å¤§è§„æ¨¡æ— æ ‡æ³¨æ–‡æœ¬");
        System.out.println("  - æŠ€å·§: è¾ƒé«˜å­¦ä¹ ç‡ + å¤šè½®è®­ç»ƒ");

        return model;
    }

    // ========== æ­¥éª¤2: ç›‘ç£å¾®è°ƒ ==========

    /**
     * æ‰§è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰- ä½¿ç”¨æ ‡å‡† SFTTrainer
     */
    private static MiniMindModel runSupervisedFinetuning(MiniMindModel pretrainedModel) throws IOException {
        System.out.println("\n" + "=".repeat(80));
        System.out.println("ğŸ¯ æ­¥éª¤2: MiniMind ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning)");
        System.out.println("=".repeat(80));

        // 1. åŠ è½½SFTæ•°æ®
        System.out.println("\nğŸ“ åŠ è½½ç›‘ç£å¾®è°ƒæ•°æ®...");
        String trainPath = DATA_DIR + "/sft_train.txt";
        List<String> trainTexts = readFromFile(trainPath);
        System.out.println("  âœ“ è®­ç»ƒé›†: " + trainTexts.size() + " æ¡");

        // 2. ä½¿ç”¨æ ‡å‡† SFTDataset
        System.out.println("\nğŸ“ å‡†å¤‡ç›‘ç£å¾®è°ƒæ•°æ®é›†...");
        MiniMindConfig config = pretrainedModel.getConfig();
        int batchSize = 2;
        
        SFTDataset dataset = new SFTDataset(sharedTokenizer, config.getMaxSeqLen(), batchSize);
        // å°†çº¯æ–‡æœ¬è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼
        for (String text : trainTexts) {
            dataset.addSample(text, "", text);  // ç®€åŒ–ï¼šæŒ‡ä»¤=è¾“å‡º
        }
        dataset.prepare(true);
        System.out.println("  âœ“ è®­ç»ƒæ ·æœ¬æ•°: " + dataset.getSampleCount());
        System.out.println("  âœ“ æ‰¹æ¬¡æ•°é‡: " + dataset.getBatchCount());

        // 3. ä½¿ç”¨æ ‡å‡† SFTTrainer
        System.out.println("\nğŸ“ å¼€å§‹ç›‘ç£å¾®è°ƒè®­ç»ƒ...");
        System.out.println("  - è®­ç»ƒç›®æ ‡: æŒ‡ä»¤è·Ÿéšå’Œå¯¹è¯ç”Ÿæˆ");
        System.out.println("  - å­¦ä¹ ç‡: 1e-3 (æ¯”é¢„è®­ç»ƒä½10å€)");
        System.out.println("  - è®­ç»ƒè½®æ¬¡: 3 epochs");
        System.out.println("-".repeat(80));

        SFTTrainer trainer = new SFTTrainer(pretrainedModel, dataset);
        trainer.configure(3, 1e-3f, 1.0f);  // 3 epochs, lr=1e-3
        trainer.train();

        System.out.println("-".repeat(80));
        System.out.println("\nâœ… ç›‘ç£å¾®è°ƒå®Œæˆ!");
        System.out.println("\nğŸ’¡ SFTé˜¶æ®µæ€»ç»“:");
        System.out.println("  - ç›®æ ‡: å­¦ä¹ éµå¾ªæŒ‡ä»¤å’Œç”Ÿæˆé«˜è´¨é‡å›ç­”");
        System.out.println("  - ä»»åŠ¡: æŒ‡ä»¤å¾®è°ƒï¼ˆé—®ç­”å¯¹ï¼‰");
        System.out.println("  - æ•°æ®: å¸¦æ ‡ç­¾çš„æŒ‡ä»¤-å›ç­”æ•°æ®");
        System.out.println("  - æŠ€å·§: å°å­¦ä¹ ç‡ + æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ");

        return pretrainedModel;
    }

    // ========== æ­¥éª¤3: å¼ºåŒ–å­¦ä¹ è®­ç»ƒ ==========

    /**
     * æ‰§è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ˆRLAIFï¼‰- ä½¿ç”¨ç®€åŒ–çš„å¥–åŠ±åŠ æƒç­–ç•¥æ¢¯åº¦
     * 
     * æ ¸å¿ƒæ€æƒ³ï¼šå°†å¥–åŠ±ä½œä¸ºæŸå¤±çš„æƒé‡ï¼Œé«˜å¥–åŠ±æ ·æœ¬è·å¾—æ›´å¤§çš„æ¢¯åº¦è´¡çŒ®
     * Loss = -reward * log P(y|x)
     */
    private static MiniMindModel runReinforcementLearningTraining(MiniMindModel finetunedModel) throws IOException {
        System.out.println("\n" + "=".repeat(80));
        System.out.println("ğŸ† æ­¥éª¤3: MiniMind å¼ºåŒ–å­¦ä¹ è®­ç»ƒ (Reinforcement Learning)");
        System.out.println("=".repeat(80));
        System.out.println("ğŸ’¡ ä½¿ç”¨å¥–åŠ±åŠ æƒçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¼˜åŒ–æ¨¡å‹");

        // 1. åŠ è½½RLæ•°æ®
        System.out.println("\nğŸ“ åŠ è½½å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®...");
        String rlPath = DATA_DIR + "/rl_train.txt";
        List<String> rlTexts = readFromFile(rlPath);
        System.out.println("  âœ“ RLè®­ç»ƒæ•°æ®: " + rlTexts.size() + " æ¡");

        // 2. è§£ææ•°æ®å¹¶æå–å¥–åŠ±
        System.out.println("\nğŸ“ å‡†å¤‡å¼ºåŒ–å­¦ä¹ æ•°æ®é›†...");
        List<String> texts = new ArrayList<>();
        List<Float> rewards = new ArrayList<>();
        
        for (String line : rlTexts) {
            float reward = extractReward(line);
            String cleanText = removeRewardLabel(line);
            texts.add(cleanText);
            rewards.add(reward);
        }
        
        float avgReward = (float) rewards.stream().mapToDouble(Float::doubleValue).average().orElse(0.0);
        System.out.println("  âœ“ RLæ ·æœ¬æ•°: " + texts.size());
        System.out.println("  âœ“ å¹³å‡å¥–åŠ±: " + String.format("%.2f", avgReward));

        // 3. é…ç½®è®­ç»ƒ
        MiniMindConfig config = finetunedModel.getConfig();
        float learningRate = 5e-4f;
        int epochs = 2;
        int logInterval = 10;
        
        System.out.println("\nğŸ“ å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ...");
        System.out.println("  - è®­ç»ƒç›®æ ‡: æœ€å¤§åŒ–å¥–åŠ±åŠ æƒçš„å¯¹æ•°æ¦‚ç‡");
        System.out.println("  - ç®—æ³•: å¥–åŠ±åŠ æƒç­–ç•¥æ¢¯åº¦ (Reward-Weighted Policy Gradient)");
        System.out.println("  - å­¦ä¹ ç‡: " + learningRate);
        System.out.println("  - è®­ç»ƒè½®æ¬¡: " + epochs);
        System.out.println("-".repeat(80));

        // 4. åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        Adam optimizer = new Adam(finetunedModel, learningRate, 0.9f, 0.999f, 1e-8f);
        SoftmaxCrossEntropy lossFunction = new SoftmaxCrossEntropy();
        finetunedModel.setTraining(true);
        
        int step = 0;
        int maxSeqLen = config.getMaxSeqLen();
        
        // 5. è®­ç»ƒå¾ªç¯
        for (int epoch = 0; epoch < epochs; epoch++) {
            float epochLoss = 0.0f;
            int sampleCount = 0;
            
            for (int i = 0; i < texts.size(); i++) {
                String text = texts.get(i);
                float reward = rewards.get(i);
                
                // ç¼–ç æ–‡æœ¬
                List<Integer> tokenIds = sharedTokenizer.encode(text, true, true);
                if (tokenIds.size() < 2) continue;
                
                // å‡†å¤‡è¾“å…¥å’Œç›®æ ‡
                int seqLen = Math.min(tokenIds.size() - 1, maxSeqLen - 1);
                float[] inputData = new float[seqLen];
                float[] targetData = new float[seqLen];
                
                for (int j = 0; j < seqLen; j++) {
                    inputData[j] = tokenIds.get(j);
                    targetData[j] = tokenIds.get(j + 1);
                }
                
                Variable input = new Variable(NdArray.of(inputData, Shape.of(1, seqLen)));
                Variable target = new Variable(NdArray.of(targetData, Shape.of(1, seqLen)));
                
                // å‰å‘ä¼ æ’­
                Variable logits = finetunedModel.predict(input);
                
                // è®¡ç®—æŸå¤± (reshapeä¸º2D)
                int[] logitsShape = logits.getValue().getShape().getShapeDims();
                int totalTokens = logitsShape[0] * logitsShape[1];
                int vocabSize = logitsShape[2];
                
                Variable logitsReshaped = logits.reshape(Shape.of(totalTokens, vocabSize));
                Variable targetReshaped = target.reshape(Shape.of(totalTokens, 1));
                
                Variable loss = lossFunction.loss(targetReshaped, logitsReshaped);
                
                // å¥–åŠ±åŠ æƒï¼šé«˜å¥–åŠ±æ ·æœ¬è·å¾—æ›´å¤§æƒé‡
                Variable weightedLoss = loss.mul(new Variable(NdArray.of(reward)));
                
                // åå‘ä¼ æ’­
                finetunedModel.clearGrads();
                weightedLoss.backward();
                optimizer.update();
                weightedLoss.unChainBackward();
                
                float lossValue = loss.getValue().getNumber().floatValue();
                epochLoss += lossValue * reward;
                sampleCount++;
                step++;
                
                if (step % logInterval == 0) {
                    System.out.printf("Epoch %d | Step %d | Loss: %.4f | Reward: %.2f%n",
                        epoch + 1, step, lossValue, reward);
                }
            }
            
            float avgLoss = sampleCount > 0 ? epochLoss / sampleCount : 0.0f;
            System.out.printf("Epoch %d å®Œæˆ | å¹³å‡åŠ æƒæŸå¤±: %.4f%n", epoch + 1, avgLoss);
        }

        System.out.println("-".repeat(80));
        System.out.println("\nâœ… å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ!");
        System.out.println("\nğŸ’¡ RLé˜¶æ®µæ€»ç»“:");
        System.out.println("  - ç›®æ ‡: é€šè¿‡å¥–åŠ±ä¿¡å·å¯¹é½æ¨¡å‹è¡Œä¸º");
        System.out.println("  - æ–¹æ³•: å¥–åŠ±åŠ æƒçš„äº¤å‰ç†µæŸå¤±");
        System.out.println("  - æ•ˆæœ: é«˜å¥–åŠ±æ ·æœ¬è·å¾—æ›´å¤§æ¢¯åº¦è´¡çŒ®");
        System.out.println("  - æŠ€å·§: å°å­¦ä¹ ç‡ + å¥–åŠ±å¼•å¯¼");

        return finetunedModel;
    }

    // ========== æ­¥éª¤4: æ¨ç†æµ‹è¯• ==========

    /**
     * æ‰§è¡Œæ¨ç†æµ‹è¯•
     */
    private static void runInference(MiniMindModel model) {
        System.out.println("\n" + "=".repeat(80));
        System.out.println("ğŸš€ æ­¥éª¤4: MiniMind æ¨ç†æµ‹è¯•");
        System.out.println("=".repeat(80));

        // è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
        model.setTraining(false);

        // æµ‹è¯•ç”¨ä¾‹
        List<String> testPrompts = Arrays.asList(
            "Question: What is machine learning?",
            "Instruction: Write a Python function",
            "Task: Explain neural networks",
            "Question: Define deep learning"
        );

        System.out.println("\nğŸ“ æµ‹è¯•ä¸åŒç”Ÿæˆç­–ç•¥...\n");

        for (String prompt : testPrompts) {
            System.out.println("æç¤ºè¯: " + prompt);

            try {
                // ç¼–ç æç¤ºè¯
                List<Integer> promptTokens = sharedTokenizer.encode(prompt);
                int[] promptIds = promptTokens.stream().mapToInt(Integer::intValue).toArray();

                // Greedyè§£ç 
                int[] greedyResult = model.generate(
                    promptIds,
                    20,      // maxNewTokens
                    0.0f,    // temperature (greedy)
                    0,       // topK
                    0.0f     // topP
                );

                String greedyText = sharedTokenizer.decode(intArrayToList(greedyResult));
                System.out.println("  [Greedy] â†’ " + greedyText);

            } catch (Exception e) {
                System.out.println("  âš  ç”Ÿæˆå¤±è´¥: " + e.getMessage());
            }

            System.out.println();
        }

        System.out.println("âœ… æ¨ç†æµ‹è¯•å®Œæˆ!");
        System.out.println("\nğŸ’¡ æ¨ç†é˜¶æ®µæ€»ç»“:");
        System.out.println("  - è¾“å…¥: æç¤ºè¯æ–‡æœ¬");
        System.out.println("  - å¤„ç†: è‡ªå›å½’ç”Ÿæˆ");
        System.out.println("  - è¾“å‡º: ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬");
        System.out.println("  - ç­–ç•¥: Greedy/Temperature/Top-K/Top-P");
    }

    // ========== è¾…åŠ©æ–¹æ³• ==========

    /**
     * åˆ›å»ºè¶…å°å‹é…ç½®ï¼ˆç”¨äºå¿«é€Ÿæ¼”ç¤ºï¼‰
     */
    private static MiniMindConfig createMicroConfig(int vocabSize) {
        MiniMindConfig config = new MiniMindConfig();
        config.setVocabSize(vocabSize);
        config.setMaxSeqLen(64);          // åºåˆ—é•¿åº¦
        config.setHiddenSize(128);        // éšè—ç»´åº¦
        config.setNumLayers(2);           // å±‚æ•°
        config.setNumHeads(4);            // æ³¨æ„åŠ›å¤´æ•°
        config.setFfnHiddenSize(256);     // FFNéšè—ç»´åº¦
        config.setDropout(0.1f);
        config.setEpsilon(1e-5f);
        return config;
    }

    /**
     * int[] è½¬ List<Integer> è¾…åŠ©æ–¹æ³•
     */
    private static List<Integer> intArrayToList(int[] array) {
        List<Integer> list = new ArrayList<>();
        for (int value : array) {
            list.add(value);
        }
        return list;
    }

    /**
     * æå–å¥–åŠ±å€¼
     */
    private static float extractReward(String text) {
        if (text.startsWith("[REWARD:")) {
            int endIdx = text.indexOf("]");
            if (endIdx > 0) {
                String rewardStr = text.substring(8, endIdx);
                try {
                    return Float.parseFloat(rewardStr);
                } catch (NumberFormatException e) {
                    return 0.5f;
                }
            }
        }
        return 0.5f;
    }

    /**
     * ç§»é™¤å¥–åŠ±æ ‡ç­¾
     */
    private static String removeRewardLabel(String text) {
        return text.replaceFirst("^\\[REWARD:[0-9.]+\\]\\s*", "");
    }

    /**
     * ä»æ–‡ä»¶è¯»å–æ–‡æœ¬
     */
    private static List<String> readFromFile(String filePath) throws IOException {
        List<String> lines = new ArrayList<>();
        try (BufferedReader reader = new BufferedReader(new FileReader(filePath))) {
            String line;
            while ((line = reader.readLine()) != null) {
                if (!line.trim().isEmpty()) {
                    lines.add(line);
                }
            }
        }
        return lines;
    }

    /**
     * å†™å…¥æ–‡ä»¶
     */
    private static void writeToFile(List<String> lines, String filePath) throws IOException {
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(filePath))) {
            for (String line : lines) {
                writer.write(line);
                writer.newLine();
            }
        }
    }
}
