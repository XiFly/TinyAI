package io.leavesfly.tinyai.nnet.block.transf;

import io.leavesfly.tinyai.func.Variable;
import io.leavesfly.tinyai.ndarr.NdArray;
import io.leavesfly.tinyai.nnet.Block;
import io.leavesfly.tinyai.nnet.block.FeedForward;
import io.leavesfly.tinyai.nnet.layer.norm.LayerNorm;
import io.leavesfly.tinyai.nnet.layer.transf.MultiHeadAttention;

import java.util.List;

/**
 * Transformer编码器层实现
 * <p>
 * 单个Transformer编码器层包含：
 * 1. 多头自注意力机制
 * 2. 残差连接和层归一化
 * 3. 前馈神经网络
 * 4. 残差连接和层归一化
 * <p>
 * EncoderLayer(x) = LayerNorm(x + FFN(LayerNorm(x + MultiHead(x))))
 */
public class TransformerEncoderBlock extends Block {

    private MultiHeadAttention selfAttention;
    private LayerNorm layerNorm1;
    private FeedForward feedForward;
    private LayerNorm layerNorm2;
    private double dropoutRate;


    public TransformerEncoderBlock(String name) {
        super(name);
    }

    /**
     * 构造Transformer编码器层
     *
     * @param name        层名称
     * @param dModel      模型维度
     * @param numHeads    注意力头数
     * @param dFF         前馈网络隐藏维度
     * @param dropoutRate dropout比率
     */
    public TransformerEncoderBlock(String name, int dModel, int numHeads, int dFF, double dropoutRate) {
        super(name);
        this.dropoutRate = dropoutRate;

        // 初始化各个子层
        this.selfAttention = new MultiHeadAttention(name + "_self_attention", dModel, numHeads, false);
        this.layerNorm1 = new LayerNorm(name + "_norm1", dModel);
        this.feedForward = new FeedForward(name + "_ffn", dModel, dFF);
        this.layerNorm2 = new LayerNorm(name + "_norm2", dModel);

        init();
    }

    /**
     * 使用默认参数的构造函数
     */
    public TransformerEncoderBlock(String name, int dModel, int numHeads) {
        this(name, dModel, numHeads, dModel * 4, 0.1);
    }

    @Override
    public void init() {
        if (!alreadyInit) {
            // 子层已经在构造函数中初始化了
            alreadyInit = true;
        }
    }

    @Override
    public Variable layerForward(Variable... inputs) {
        Variable x = inputs[0];

        // 1. 多头自注意力 + 残差连接 + 层归一化
        Variable attentionOutput = selfAttention.layerForward(x, x, x);
        Variable residual1 = addResidualConnection(x, attentionOutput);
        Variable norm1Output = layerNorm1.layerForward(residual1);

        // 2. 前馈网络 + 残差连接 + 层归一化
        Variable ffnOutput = feedForward.layerForward(norm1Output);
        Variable residual2 = addResidualConnection(norm1Output, ffnOutput);
        Variable norm2Output = layerNorm2.layerForward(residual2);

        return norm2Output;
    }

    /**
     * 添加残差连接
     */
    private Variable addResidualConnection(Variable input, Variable output) {
        // 残差连接：input + output
        return input.add(output);
    }

    /**
     * 应用Dropout（简化版本）
     * 在实际实现中，需要考虑训练/推理模式
     */
    private Variable applyDropout(Variable input) {
        if (dropoutRate > 0.0) {
            // 这里是简化的dropout实现
            // 实际应用中需要生成随机掩码并考虑训练/推理模式
            return input;
        }
        return input;
    }


    @Override
    public NdArray forward(NdArray... inputs) {
        return null;
    }

    @Override
    public List<NdArray> backward(NdArray yGrad) {
        return null;
    }

    @Override
    public int requireInputNum() {
        return 1;
    }

    /**
     * 获取自注意力层
     */
    public MultiHeadAttention getSelfAttention() {
        return selfAttention;
    }

    /**
     * 获取第一个层归一化层
     */
    public LayerNorm getLayerNorm1() {
        return layerNorm1;
    }

    /**
     * 获取前馈网络层
     */
    public FeedForward getFeedForward() {
        return feedForward;
    }

    /**
     * 获取第二个层归一化层
     */
    public LayerNorm getLayerNorm2() {
        return layerNorm2;
    }
}